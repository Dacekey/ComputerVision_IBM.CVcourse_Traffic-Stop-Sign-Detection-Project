{"cells":[{"cell_type":"markdown","id":"6f678226-7139-4274-8b9d-0873e1c0f80a","metadata":{},"outputs":[],"source":["<p>\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","id":"29ed4a03-4a4c-4ada-bb06-df4bb78d1cc9","metadata":{},"outputs":[],"source":["<h1> Lab: Geometric Transformations with OpenCV</h1>\n"]},{"cell_type":"markdown","id":"79f846b3-8678-4f30-bb6f-9c5ac3e85b7c","metadata":{},"outputs":[],"source":["Estimated time needed: **40** minutes\n"]},{"cell_type":"markdown","id":"2659ff9f-48dc-4b6f-ba2c-0c7c4b71e0af","metadata":{},"outputs":[],"source":["<h2>Objectives</h2>\n"]},{"cell_type":"markdown","id":"1b64f92a-725f-49de-ac4f-f716f514201e","metadata":{},"outputs":[],"source":["In the first part of the lab, you will apply geometric transformations to an image. This allows you to perform different operations like reshape translation i.e. to shift, reshape and rotate the image. In the second part of the lab, you will learn how to apply some basic array and matrix operations to the image. \n"]},{"cell_type":"markdown","id":"eb0c8204-c73f-43ba-8e05-179860a8a20c","metadata":{},"outputs":[],"source":["Table of Contents:\n","- [Geometric Transformations](#Geometric-Transformations)\n","- [Mathematical Operations](#Mathematical-Operations)\n"]},{"cell_type":"markdown","id":"5cbf63c5-d33c-42d9-a944-a8b81a4b9d4b","metadata":{},"outputs":[],"source":["----\n"]},{"cell_type":"markdown","id":"091ab478-5499-4c11-ae33-6933b7f8ec0b","metadata":{},"outputs":[],"source":["Install required libraries:\n"]},{"cell_type":"code","id":"254ecfc3-56e3-45b0-8860-f8346d352234","metadata":{},"outputs":[],"source":["!pip install opencv-python-headless\n!pip install matplotlib"]},{"cell_type":"markdown","id":"1f65c2ff-19a5-4a9e-af37-a77c0521212c","metadata":{},"outputs":[],"source":["Download the image for the lab:\n"]},{"cell_type":"code","id":"a89053d5-4065-4160-b8ac-1c8a3968e297","metadata":{},"outputs":[],"source":["!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/lenna.png -O lenna.png\n!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/baboon.png -O baboon.png\n!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images%20/images_part_1/barbara.png -O barbara.png  "]},{"cell_type":"markdown","id":"e22693f1-5bbc-4b97-a45f-cc64db546f76","metadata":{},"outputs":[],"source":["We will import the following:\n"]},{"cell_type":"code","id":"3b8507ee-94a6-46d4-9fad-ae871166cf9d","metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\nimport cv2\nimport numpy as np"]},{"cell_type":"markdown","id":"a0192fa5-523a-4679-be39-59d51896a3b5","metadata":{},"outputs":[],"source":["First, let's define a helper function to plot two images side-by-side. You will not need to understand this code this moment, but this function will be used repeatedly in this tutorial to showcase the results. \n"]},{"cell_type":"code","id":"8a80414c-78db-4555-94eb-7c1f25017717","metadata":{},"outputs":[],"source":["def plot_image(image_1, image_2,title_1=\"Orignal\",title_2=\"New Image\"):\n    plt.figure(figsize=(10,10))\n    plt.subplot(1, 2, 1)\n    plt.imshow(image_1,cmap=\"gray\")\n    plt.title(title_1)\n    plt.subplot(1, 2, 2)\n    plt.imshow(image_2,cmap=\"gray\")\n    plt.title(title_2)\n    plt.show()"]},{"cell_type":"markdown","id":"bb293f28-1e3b-45cd-b8fd-ce0f539d93b9","metadata":{},"outputs":[],"source":["#  Geometric Transformations\n"]},{"cell_type":"markdown","id":"c833e489-5c39-4ed1-8c33-7f23f9121c6a","metadata":{},"outputs":[],"source":[" Geometric transformations allow you to perform different operations like translation i.e. to shift, reshape and rotate the image.\n"]},{"cell_type":"markdown","id":"02e03208-53f6-4733-95d4-fd6a89f1de53","metadata":{},"outputs":[],"source":["## Scaling \n"]},{"cell_type":"markdown","id":"0f244c89-c193-45a8-a1e4-99102a58acd0","metadata":{},"outputs":[],"source":["We can resize an image using the function `resize()` from `cv2` module for this purpose.  You can specify the scaling factor or the size of the image:\n"]},{"cell_type":"markdown","id":"7f14b734-0ee3-4df9-a1f9-926ab09c9917","metadata":{},"outputs":[],"source":["Consider the following image with the corresponding intensity values:\n"]},{"cell_type":"code","id":"de143b3e-a110-4283-aab5-122060768910","metadata":{},"outputs":[],"source":["toy_image = np.zeros((6,6))\ntoy_image[1:5,1:5]=255\ntoy_image[2:4,2:4]=0\nplt.imshow(toy_image,cmap='gray')\nplt.show()\ntoy_image"]},{"cell_type":"markdown","id":"47edd2b4-82fd-465d-a695-56cc25b5682c","metadata":{},"outputs":[],"source":["We can rescale along a specific axis:\n","\n","- `fx`: scale factor along the horizontal axis  \n","- `fy`: scale factor along the vertical axis\n"]},{"cell_type":"markdown","id":"245d74c7-0fba-4f48-8fb7-a73658996c30","metadata":{},"outputs":[],"source":["The parameter interpolation estimates pixel values based on neighboring pixels. <code>INTER_NEAREST</code> uses the nearest pixel and <code>INTER_CUBIC</code> uses several pixels near the pixel value we would like to estimate.\n"]},{"cell_type":"code","id":"3a8b162c-8746-47ab-b9ed-a0a983e01807","metadata":{},"outputs":[],"source":["new_toy = cv2.resize(toy_image,None,fx=2, fy=1, interpolation = cv2.INTER_NEAREST )\nplt.imshow(new_toy,cmap='gray')\nplt.show()\n"]},{"cell_type":"markdown","id":"9adc096b-254f-4771-97b6-aff1c2e9932b","metadata":{},"outputs":[],"source":["Consider the following image:\n"]},{"cell_type":"code","id":"c4f0de41-2618-4368-a28e-52e9ee070213","metadata":{},"outputs":[],"source":["image = cv2.imread(\"lenna.png\")\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()"]},{"cell_type":"markdown","id":"7d7ecde0-032c-47ba-a8d6-c3d5e21122c2","metadata":{},"outputs":[],"source":["We can scale the horizontal axis by two and leave the vertical axis as is:\n"]},{"cell_type":"code","id":"8881b119-c10e-43d9-98e9-f8bc0799aa8e","metadata":{},"outputs":[],"source":["new_image = cv2.resize(image, None, fx=2, fy=1, interpolation=cv2.INTER_CUBIC)\nplt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB))\nplt.show()\nprint(\"old image shape:\", image.shape, \"new image shape:\", new_image.shape)"]},{"cell_type":"markdown","id":"dd7f69c7-9e12-417a-bb26-902278fe8abb","metadata":{},"outputs":[],"source":["In the same manner, we can scale the vertical axis by two:\n"]},{"cell_type":"code","id":"56904924-b2fb-4a9b-96b5-089ec42e46fd","metadata":{},"outputs":[],"source":["new_image = cv2.resize(image, None, fx=1, fy=2, interpolation=cv2.INTER_CUBIC)\nplt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB))\nplt.show()\nprint(\"old image shape:\", image.shape, \"new image shape:\", new_image.shape)"]},{"cell_type":"markdown","id":"b7720eeb-bdb0-4201-b39d-f99cb71640c4","metadata":{},"outputs":[],"source":["We can scale the horizontal axis and vertical axis by two.\n"]},{"cell_type":"code","id":"9034847a-9074-474e-a0b0-9df210d7e604","metadata":{},"outputs":[],"source":["new_image = cv2.resize(image, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\nplt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB))\nplt.show()\nprint(\"old image shape:\", image.shape, \"new image shape:\", new_image.shape)"]},{"cell_type":"markdown","id":"a2e297d7-fa45-466b-ad29-07fcbddcf7c7","metadata":{},"outputs":[],"source":["We can also shrink the image by setting the scaling factor to a real number between 0 and 1:\n"]},{"cell_type":"code","id":"27b96c31-1d67-48bf-ae13-4deb1f5b9e3d","metadata":{},"outputs":[],"source":["new_image = cv2.resize(image, None, fx=1, fy=0.5, interpolation=cv2.INTER_CUBIC)\nplt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB))\nplt.show()\nprint(\"old image shape:\", image.shape, \"new image shape:\", new_image.shape)"]},{"cell_type":"markdown","id":"171dfb72-a27d-403d-884e-8d555806ace6","metadata":{},"outputs":[],"source":["We can  also specify the number of rows and columns:\n"]},{"cell_type":"code","id":"50890d55-73fd-4016-b64a-364db060c002","metadata":{},"outputs":[],"source":["rows = 100\ncols = 200"]},{"cell_type":"code","id":"36c115ab-0417-4e15-8706-058d66bdde2d","metadata":{},"outputs":[],"source":["new_image = cv2.resize(image, (100, 200), interpolation=cv2.INTER_CUBIC)\nplt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB))\nplt.show()\nprint(\"old image shape:\", image.shape, \"new image shape:\", new_image.shape)"]},{"cell_type":"markdown","id":"a20d8295-ac7d-4760-8f36-8d5abf04c7e3","metadata":{},"outputs":[],"source":["## Translation\n"]},{"cell_type":"markdown","id":"17a94683-cb3e-4545-b346-4d3830032559","metadata":{},"outputs":[],"source":["Translation is  when you  shift the location of the image. <code>tx</code> is the number of pixels you shift the location in the horizontal direction and <code>ty</code> is the number of pixels you shift in the vertical direction. You can create the transformation matrix $M$ to shift the image. \n","\n","In this example, we shift the image 100 pixels horizontally:\n"]},{"cell_type":"code","id":"b88a7984-8f78-4a2a-b000-917f434413ea","metadata":{},"outputs":[],"source":["tx = 100\nty = 0\nM = np.float32([[1, 0, tx], [0, 1, ty]])\nM"]},{"cell_type":"markdown","id":"5e5b1f37-392a-4b7f-b500-c5b1fd98abbe","metadata":{},"outputs":[],"source":["The shape of the image is given by:\n"]},{"cell_type":"code","id":"0b5d40f2-d3de-4b2f-95b8-f1b22eca27c0","metadata":{},"outputs":[],"source":["rows, cols, _ = image.shape"]},{"cell_type":"markdown","id":"c5c426a8-115b-48d6-928c-da430e366085","metadata":{},"outputs":[],"source":["We use the function <code>warpAffine</code> from the <code>cv2</code> module. The first input parater is an image array, the second input parameter is the transformation matrix <code>M</code>, and the final input paramter is the length and width of the output image $(cols,rows)$:\n"]},{"cell_type":"code","id":"d5f88bbc-aa76-40ca-844e-371226d9984c","metadata":{},"outputs":[],"source":["new_image = cv2.warpAffine(image, M, (cols, rows))"]},{"cell_type":"markdown","id":"0a4fbc4d-5019-47f4-b789-17d0fd535d1d","metadata":{},"outputs":[],"source":["We can plot the image; the portions of the image that do not have any intensities are set to zero:\n"]},{"cell_type":"code","id":"78543495-f922-4b79-95be-a35c89474855","metadata":{},"outputs":[],"source":["plt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB))\nplt.show()"]},{"cell_type":"markdown","id":"6836c941-6ff1-49e4-9c2b-4eddc2a36be9","metadata":{},"outputs":[],"source":["We can see some of the original image has been cut off. We can fix this by changing the output image size: <code>(cols + tx,rows + ty)</code>:\n"]},{"cell_type":"code","id":"e4f361fc-049c-497e-a273-d1048e7caffb","metadata":{},"outputs":[],"source":["new_image = cv2.warpAffine(image, M, (cols + tx, rows + ty))\nplt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB))\nplt.show()"]},{"cell_type":"markdown","id":"d17534ed-8d18-40a4-a1f5-23ec6e2aa9fe","metadata":{},"outputs":[],"source":["We can shift the image horizontally:\n"]},{"cell_type":"code","id":"c5ed1252-6285-4548-b3bc-40c925b1e9ac","metadata":{},"outputs":[],"source":["tx = 0\nty = 50\nM = np.float32([[1, 0, tx], [0, 1, ty]])\nnew_iamge = cv2.warpAffine(image, M, (cols + tx, rows + ty))\nplt.imshow(cv2.cvtColor(new_iamge, cv2.COLOR_BGR2RGB))\nplt.show()"]},{"cell_type":"markdown","id":"93d356a8-8d15-4e15-8dfc-d4c401785364","metadata":{},"outputs":[],"source":["## Rotation \n"]},{"cell_type":"markdown","id":"3820c7f8-1412-4bb2-acd4-6a13b5daa14b","metadata":{},"outputs":[],"source":["We can rotate an image by angle θ which is achieved by the Rotation Matrix <code>getRotationMatrix2D</code>.\n"]},{"cell_type":"markdown","id":"e83a0986-94d5-41b0-9349-555937134131","metadata":{},"outputs":[],"source":["<p><code>center</code>: Center of the rotation in the source image. We will only use the center of the image.</p>\n","<p><code>angle</code>: Rotation angle in degrees. Positive values mean counter-clockwise rotation (the coordinate origin is assumed to be the top-left corner).</p>\n","<p><code>scale</code>: Isotropic scale factor, in this course the value will be one.</p>\n"]},{"cell_type":"markdown","id":"94d469b2-f84f-4d22-b504-eaea10e3a284","metadata":{},"outputs":[],"source":["We can rotate our toy image by 45 degrees:\n"]},{"cell_type":"code","id":"79c06423-17c5-4e7e-bf09-30811280af94","metadata":{},"outputs":[],"source":["theta = 45.0\nM = cv2.getRotationMatrix2D(center=(3, 3), angle=theta, scale=1)\nnew_toy_image = cv2.warpAffine(toy_image, M, (6, 6))"]},{"cell_type":"code","id":"aba9f318-7e20-4dcf-92a5-ad9dd04e13bd","metadata":{},"outputs":[],"source":["plot_image(toy_image, new_toy_image, title_1=\"Orignal\", title_2=\"rotated image\")"]},{"cell_type":"markdown","id":"981eff74-a521-4233-be2f-6842b34c2f56","metadata":{},"outputs":[],"source":["Looking at intensity values, we see that many values have been interpolated:\n"]},{"cell_type":"code","id":"a78ccb38-5abf-4dfe-831f-9e1cc4cde43f","metadata":{},"outputs":[],"source":["new_toy_image "]},{"cell_type":"markdown","id":"484eb605-e26a-4870-8b56-42c5c9aef140","metadata":{},"outputs":[],"source":["We can perform the same operation on color images:\n"]},{"cell_type":"code","id":"dcaa6e46-4614-434e-b790-4d1bbddbf421","metadata":{},"outputs":[],"source":["cols, rows, _ = image.shape"]},{"cell_type":"code","id":"0b2bbdf3-a964-4b3e-a523-f580033ea008","metadata":{},"outputs":[],"source":["M = cv2.getRotationMatrix2D(center=(cols // 2 - 1, rows // 2 - 1), angle=theta, scale=1)\nnew_image = cv2.warpAffine(image, M, (cols, rows))"]},{"cell_type":"code","id":"0aa4cde0-c40a-451b-b5a7-ea352489f73a","metadata":{},"outputs":[],"source":["plt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB))\nplt.show()"]},{"cell_type":"markdown","id":"6c60b3c5-e189-4c3b-a89e-1c3bf614e993","metadata":{},"outputs":[],"source":["# Mathematical Operations \n"]},{"cell_type":"markdown","id":"c53c77a9-ee84-463a-9668-589628023a5b","metadata":{},"outputs":[],"source":["## Array Operations \n"]},{"cell_type":"markdown","id":"efe45856-b2d2-429b-827f-38f4655ceab5","metadata":{},"outputs":[],"source":["We can perform array operations on an image; Using Python broadcasting, we can add a constant to each pixel's intensity value. \n"]},{"cell_type":"code","id":"53a0415e-a2ed-43a0-9c2c-1bb04cda9a8f","metadata":{},"outputs":[],"source":["new_image = image + 20\n\nplt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB))\nplt.show()"]},{"cell_type":"markdown","id":"2e9aa6a2-e86b-4f18-be98-cfff18d6d2fc","metadata":{},"outputs":[],"source":["We can also multiply every pixel's intensity value by a constant value.\n"]},{"cell_type":"code","id":"35e25917-ea6e-44fd-8cc8-5012f8d24ca3","metadata":{},"outputs":[],"source":["new_image = 10 * image\nplt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB))\nplt.show()"]},{"cell_type":"markdown","id":"9168fdf2-9c31-4fd6-b0e0-f1757cb2ef7e","metadata":{},"outputs":[],"source":["We can add the elements of two arrays of equal shape. In this example, we generate an array of random noises with the same shape and data type as our image.\n"]},{"cell_type":"code","id":"88af7cd2-c030-4f40-a38c-04c591128988","metadata":{},"outputs":[],"source":["Noise = np.random.normal(0, 20, (rows, cols, 3)).astype(np.uint8)\nNoise.shape\n"]},{"cell_type":"markdown","id":"65b8e360-f376-481d-ac3c-5bda7bcb26a5","metadata":{},"outputs":[],"source":["We add the generated noise to the image and plot the result. We see the values that have corrupted the image:\n"]},{"cell_type":"code","id":"9c47f585-a891-4047-b73d-724138b927f8","metadata":{},"outputs":[],"source":["new_image = image + Noise\n\nplt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB))\nplt.show()"]},{"cell_type":"markdown","id":"41685552-787c-480d-9ffa-e7dad9f1bdf9","metadata":{},"outputs":[],"source":["At the same time, we can multiply the elements of two arrays of equal shape. We can multiply the random image and the Lenna image and plot the result. \n"]},{"cell_type":"code","id":"aaf4aab4-df9d-41ff-9845-85c48987e2bb","metadata":{},"outputs":[],"source":["new_image = image*Noise\n\nplt.imshow(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB))\nplt.show()"]},{"cell_type":"markdown","id":"039d3be9-867a-4feb-b9e7-bf52ac6f11b0","metadata":{},"outputs":[],"source":["## Matrix Operations \n"]},{"cell_type":"markdown","id":"ebf6520d-7d3e-4594-9ada-0377a30b18c0","metadata":{},"outputs":[],"source":["Grayscale images are matrices. Consider the following grayscale image:\n"]},{"cell_type":"code","id":"d3fc3a74-43af-45fa-8b43-950495376a4c","metadata":{},"outputs":[],"source":["im_gray = cv2.imread('barbara.png', cv2.IMREAD_GRAYSCALE)\nim_gray.shape\n\nplt.imshow(im_gray,cmap='gray')\nplt.show()"]},{"cell_type":"markdown","id":"adbe5600-5cec-4602-b18e-830f47abf4e0","metadata":{},"outputs":[],"source":["We can apply algorithms designed for matrices.  We can use  Singular Value Decomposition, decomposing our image matrix into  a product of three matrices.\n"]},{"cell_type":"code","id":"8267f9e8-6de9-45e0-bef5-c0bcde2d9b97","metadata":{},"outputs":[],"source":["U, s, V = np.linalg.svd(im_gray , full_matrices=True)"]},{"cell_type":"markdown","id":"269e5299-25c1-41e3-8b2f-bd90cb02c41f","metadata":{},"outputs":[],"source":["We see <code>s</code> is not rectangular:\n"]},{"cell_type":"code","id":"e7b22ed3-a2d2-4f8f-8443-73308a5919b4","metadata":{},"outputs":[],"source":["s.shape"]},{"cell_type":"markdown","id":"e93938aa-cb73-427a-bf6e-8ca4fd8df8f3","metadata":{},"outputs":[],"source":["We can convert  <code>s</code> to a diagonal matrix <code>S</code>:\n"]},{"cell_type":"code","id":"d66d0dd2-da5b-40b3-92e9-99037a8d45bd","metadata":{},"outputs":[],"source":["S = np.zeros((im_gray.shape[0], im_gray.shape[1]))\nS[:image.shape[0], :image.shape[0]] = np.diag(s)\n"]},{"cell_type":"markdown","id":"256ddf71-da39-4eb7-a892-038e8b84ced8","metadata":{},"outputs":[],"source":["We can plot the matrix `U` and `V`:\n"]},{"cell_type":"code","id":"3d5b6d32-66ab-43e0-b2aa-b3a380a1823e","metadata":{},"outputs":[],"source":[" plot_image(U,V,title_1=\"Matrix U \",title_2=\"matrix  V\")"]},{"cell_type":"markdown","id":"cfb7d1ba-cbbc-404a-85f8-94e2620dc2fc","metadata":{},"outputs":[],"source":["We see most of the elements in `S` are zero:\n"]},{"cell_type":"code","id":"caec7b54-9ae1-4510-af42-b37b53054a01","metadata":{},"outputs":[],"source":["plt.imshow(S,cmap='gray')\nplt.show()"]},{"cell_type":"markdown","id":"a0501000-15df-4bb8-9d74-90a756e6ad7d","metadata":{},"outputs":[],"source":["We can find the matrix product of all the matrices. First, we can perform matrix multiplication on `S` and `U` and assign it  to `B` and plot the results: \n"]},{"cell_type":"code","id":"e78f9ba4-a932-43ab-8029-eda8a99ab792","metadata":{},"outputs":[],"source":["B = S.dot(V)\nplt.imshow(B,cmap='gray')\nplt.show()"]},{"cell_type":"markdown","id":"0235f214-7ac5-453f-8174-f355a449f508","metadata":{},"outputs":[],"source":["We can find the matrix product of `U`, `S`, and `B`. We see it’s the entire image:\n"]},{"cell_type":"code","id":"64f47674-6b63-4eca-874c-3dfbf0a913c9","metadata":{},"outputs":[],"source":["A = U.dot(B)"]},{"cell_type":"code","id":"97788893-d349-4aa7-a729-73c45e1adf76","metadata":{},"outputs":[],"source":["plt.imshow(A,cmap='gray')\nplt.show()"]},{"cell_type":"markdown","id":"bed1f62e-d09d-4aaa-8a37-e46a38f3e119","metadata":{},"outputs":[],"source":["It turns out many elements are redundant, so we can eliminate some rows and columns of `S` and `V` and approximate the image by finding the product.\n"]},{"cell_type":"code","id":"8d6aba8a-6bef-4050-b378-f79aee213319","metadata":{},"outputs":[],"source":["for n_component in [1,10,100,200, 500]:\n    S_new = S[:, :n_component]\n    V_new = V[:n_component, :]\n    A = U.dot(S_new.dot(V_new))\n    plt.imshow(A,cmap='gray')\n    plt.title(\"Number of Components:\"+str(n_component))\n    plt.show()"]},{"cell_type":"markdown","id":"c8197bb7-e88f-417e-a3f9-e767aa53beaf","metadata":{},"outputs":[],"source":["We see we only need 100 to 200 Components to represent the image.\n"]},{"cell_type":"markdown","id":"c27c88c9-2792-485f-84b9-8618ba8dabf4","metadata":{},"outputs":[],"source":["<h2>Authors</h2>\n"]},{"cell_type":"markdown","id":"6e91fac2-782a-4954-82a0-a45c62913590","metadata":{},"outputs":[],"source":[" [Joseph Santarcangelo]( https://www.linkedin.com/in/joseph-s-50398b136/) has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"]},{"cell_type":"markdown","id":"6a301092-1c87-4ccb-a16d-60beaab78d6c","metadata":{},"outputs":[],"source":["# References \n"]},{"cell_type":"markdown","id":"26044ddb-7bc8-41b4-afd5-18ce7a06ba37","metadata":{},"outputs":[],"source":["[1]  Images were taken from: https://homepages.cae.wisc.edu/~ece533/images/\n","    \n","[2]  <a href='https://pillow.readthedocs.io/en/stable/index.html'>Pillow Docs</a>\n","\n","[3]  <a href='https://opencv.org/'>Open CV</a>\n","\n","[4] Gonzalez, Rafael C., and Richard E. Woods. \"Digital image processing.\" (2017).\n","\n","[5 ] Jian, Wushuai, Xueyan Sun, and Shuqian Luo. \"Computer-aided diagnosis of breast microcalcifications based on dual-tree complex wavelet transform.\" Biomedical engineering online 11.1 (2012): 1-12.\n"]},{"cell_type":"markdown","id":"d3953a93-73db-49d2-9fd4-71f07234b7fd","metadata":{},"outputs":[],"source":["<!--<h2>Change Log</h2>-->\n"]},{"cell_type":"markdown","id":"b1cfc1be-42bd-4b77-9175-754c056ff8ca","metadata":{},"outputs":[],"source":["<!--\n","<table>\n","    <tr>\n","        <th>Date (YYYY-MM-DD)</th>\n","        <th>Version</th>\n","        <th>Changed By</th>\n","        <th>Change Description</th>\n","    </tr>\n","    <tr>\n","        <td>2020-07-20</td>\n","        <td>0.2</td>\n","        <td>Azim</td>\n","        <td>Modified Multiple Areas</td>\n","    </tr>\n","    <tr>\n","        <td>2020-07-17</td>\n","        <td>0.1</td>\n","        <td>Azim</td>\n","        <td>Created Lab Template</td>\n","    </tr>\n","</table>\n","-->\n"]},{"cell_type":"markdown","id":"90a512ab-0c97-4b33-add0-c24298ff5984","metadata":{},"outputs":[],"source":["\n","<h3 align=\"center\"> &#169; IBM Corporation. All rights reserved. <h3/>\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.8"},"prev_pub_hash":"197e80ab4440f487a846c741e23d364f1e3fafa0069dfaa27cf16675f0e6c6cb"},"nbformat":4,"nbformat_minor":4}