{"cells":[{"cell_type":"markdown","id":"c7acefde-a4c2-40eb-92c2-6df92f703eb5","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","id":"7efdee79-01f1-4b18-b415-1a592a4c28a1","metadata":{},"outputs":[],"source":["<h2>Lab: Image Classification with CNN\n"]},{"cell_type":"markdown","id":"d90c21fb-f1b7-4ea4-8dd2-0b689760f379","metadata":{},"outputs":[],"source":["Estimated time needed: **60** minutes\n"]},{"cell_type":"markdown","id":"98559b37-1594-42c3-ad01-3f1b24f4df73","metadata":{},"outputs":[],"source":["## Overview\n"]},{"cell_type":"markdown","id":"7aaa2d97-23e3-4e62-886c-ab690dabf25e","metadata":{},"outputs":[],"source":["In this lab, you will train a deep neural network for  image classification using <a href=\"https://cs231n.github.io/transfer-learning/\">transfer learning</a>. Experiment with different hyperparameters.\n"]},{"cell_type":"markdown","id":"86ffc76e-53e5-43bf-b7c9-afeccbfe743e","metadata":{},"outputs":[],"source":["## Objectives\n"]},{"cell_type":"markdown","id":"31959282-efec-43cd-9e92-40cdf95ecf20","metadata":{},"outputs":[],"source":["In this lab, you will train a state-of-the-art image classifier. In practice, very few people train an entire Convolutional Neural Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size and training from scratch is resource-intensive.\n","Instead, it is common to use a ConvNet that has already been pretrained on a very large dataset (such as ResNet), and then fine-tune it on your own dataset. We will use the pretrained Convolutional Network as a feature extractor, training only the output layer.\n","In general, 100–200 images will give you a good starting point, and it only takes about half an hour. Usually, the more images you add, the better your results, but it takes longer, and the rate of improvement will decrease.\n"]},{"cell_type":"markdown","id":"94541465-0a9b-4c67-82a5-e5de97357b6b","metadata":{},"outputs":[],"source":["# Table of content\n"]},{"cell_type":"markdown","id":"0e90d10f-2e60-4b9d-85c9-523ab3287298","metadata":{},"outputs":[],"source":["This notebook is organized into the following sections:\n","-   [Install and Import Libraries](#Install-and-Import-Libraries)\n","-   [Image Processing and Load Data for Dataset preparation](#Image-Processing-and-Load-Data-for-Dataset-preparation)\n","-   [Load Model and Train](#Load-Model-and-Train)\n","-   [Practice Exercise](#Practice-Exercise)\n"]},{"cell_type":"markdown","id":"99d77fbd-3440-4da9-9826-527fc63b7668","metadata":{},"outputs":[],"source":["* * *\n"]},{"cell_type":"markdown","id":"955c9d30-a0f9-4fa4-8114-5d5a13c5c865","metadata":{},"outputs":[],"source":["## Install and Import Libraries\n"]},{"cell_type":"markdown","id":"3f29601f-f372-498c-86e4-3e6736215c4d","metadata":{},"outputs":[],"source":["**It may take time for installation so please be patient.**\n"]},{"cell_type":"code","id":"62d3f4ee-823e-44bb-8972-5cb947313de2","metadata":{},"outputs":[],"source":["# Core libraries\n!pip install numpy pandas matplotlib tqdm pillow --quiet\n"]},{"cell_type":"code","id":"a0b60898-9c5a-43f6-a123-f35c75e4e3e2","metadata":{},"outputs":[],"source":["# Widgets for progress bars and interactivity\n!pip install ipywidgets --quiet\n# If you use JupyterLab, also enable the widget extension\n#!jupyter nbextension enable --py widgetsnbextension"]},{"cell_type":"code","id":"51c8cd42-9096-4846-9c2c-47386482e186","metadata":{},"outputs":[],"source":["# PyTorch (for CPU)\n!pip install torch torchvision --quiet"]},{"cell_type":"markdown","id":"895799b3-4a13-43df-abd2-4497b238eb03","metadata":{},"outputs":[],"source":["**Import Libraries and Define Auxiliary Functions**\n"]},{"cell_type":"markdown","id":"d28b1a4b-a9e2-447b-ae89-1bd2b17837cd","metadata":{},"outputs":[],"source":["Libraries for OS and Cloud\n"]},{"cell_type":"code","id":"2acaf4db-4abc-43f8-a2fb-ceb77e8e0d4b","metadata":{},"outputs":[],"source":["import os\nimport uuid\nimport shutil\nimport json\nimport copy\nfrom datetime import datetime\nimport zipfile\nimport io\nimport requests\nimport random"]},{"cell_type":"markdown","id":"25cd9884-0314-4230-9a16-a81fcc1b6329","metadata":{},"outputs":[],"source":["Libraries for Data Processing and Visualization\n"]},{"cell_type":"code","id":"9db7c3d3-d015-451f-885a-b81116425910","metadata":{},"outputs":[],"source":["from PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport math\nfrom matplotlib.pyplot import imshow\nfrom tqdm import tqdm\nfrom ipywidgets import IntProgress\nimport time "]},{"cell_type":"markdown","id":"2def12ec-d204-42e6-8aac-a1d5445ef36e","metadata":{},"outputs":[],"source":["Deep Learning Libraries\n"]},{"cell_type":"code","id":"aa9a9993-8793-43ff-827e-aea54a63cf2b","metadata":{},"outputs":[],"source":["import torch\nimport torchvision.models as models\nfrom torch.utils.data import Dataset, DataLoader,random_split\nfrom torch.optim import lr_scheduler\nfrom torchvision import transforms\nimport torch.nn as nn\ntorch.manual_seed(0)\nfrom torchvision.datasets import ImageFolder\nfrom torchvision import transforms"]},{"cell_type":"markdown","id":"30cefc95-8b7a-4db0-a92d-d78f1276436b","metadata":{},"outputs":[],"source":["**Plot train cost and validation accuracy:**\n","\n","The `plot_stuff` function is used to visualize the model's training performance by plotting training loss and validation accuracy on the same graph using two different y-axes. \n","\n","The training loss, shown in red on the left y-axis, indicates how well the model is minimizing error during training. \n","\n","The validation accuracy, shown in blue on the right y-axis, reflects how well the model is performing on unseen data. \n","\n","By plotting both metrics together, this function helps monitor the learning process, identify trends, and detect issues such as overfitting—where the model performs well on training data but poorly on validation data. \n","This kind of visualization is valuable for understanding the effectiveness of your training strategy and making informed adjustments to model architecture or hyperparameters.\n"]},{"cell_type":"code","id":"cc3f68a1-62b3-4f87-acfe-9eebf40ae7c2","metadata":{},"outputs":[],"source":["def plot_stuff(COST, ACC):\n    \"\"\"\n    Plots training cost (loss) and validation accuracy on the same figure using two y-axes.\n    \n    Parameters:\n    COST (list or array): Total training loss per iteration (or epoch)\n    ACC (list or array): Validation accuracy per iteration (or epoch)\n    \"\"\"\n    \n    # Create a new figure and a primary axis (ax1)\n    fig, ax1 = plt.subplots()\n    \n    # Plot training loss on the primary y-axis (left)\n    color = 'tab:red'\n    ax1.plot(COST, color=color)\n    ax1.set_xlabel('Iteration', color=color)            # Label for x-axis\n    ax1.set_ylabel('Total Loss', color=color)           # Label for y-axis (left)\n    ax1.tick_params(axis='y', labelcolor=color)         # Set y-axis tick color\n\n    # Create a secondary y-axis (ax2) sharing the same x-axis\n    ax2 = ax1.twinx()\n    \n    # Plot validation accuracy on the secondary y-axis (right)\n    color = 'tab:blue'\n    ax2.set_ylabel('Accuracy', color=color)             # Label for y-axis (right)\n    ax2.plot(ACC, color=color)\n    ax2.tick_params(axis='y', labelcolor=color)\n\n    # Adjust layout to prevent y-label clipping\n    fig.tight_layout()\n    \n    # Display the combined plot\n    plt.show()\n"]},{"cell_type":"markdown","id":"c517a745-1702-4eb0-8f1b-5d3bb782ca1e","metadata":{},"outputs":[],"source":["**Plot the transformed image:**\n","\n","When using `transforms.Normalize` during preprocessing (e.g., for pretrained CNNs like ResNet), images are normalized with mean and standard deviation values. But this makes the image look odd if displayed directly.\n","\n","This function reverses that normalization so the image appears as a proper RGB image for visualization, especially useful for:\n","\n","Checking your image pipeline.\n","\n","Displaying samples from datasets.\n","\n","Debugging predictions or model outputs.\n"]},{"cell_type":"code","id":"6594575d-bd56-4c18-a5b5-3cdddb5fbff9","metadata":{},"outputs":[],"source":["def imshow_(inp, title=None):\n    \"\"\"\n    Displays a tensor image after reversing normalization.\n    \n    Parameters:\n    - inp (Tensor): Image tensor of shape [C, H, W], usually normalized.\n    - title (str, optional): Title for the image display.\n    \"\"\"\n    # Convert from [C, H, W] to [H, W, C] and to NumPy array\n    inp = inp.permute(1, 2, 0).numpy()\n    print(\"Image shape:\", inp.shape)\n\n    # Undo normalization (ImageNet mean and std)\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n\n    # Clip values to [0, 1] range for display\n    inp = np.clip(inp, 0, 1)\n\n    # Display image\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # Short pause for GUI update\n    plt.show()\n"]},{"cell_type":"markdown","id":"9c86bf3e-7bae-4ebe-8fcc-a179686b3b62","metadata":{},"outputs":[],"source":["**Define our device as the first visible cuda device if we have CUDA available:**\n"]},{"cell_type":"code","id":"b43b130c-a265-4674-bf6c-83175ecba4b3","metadata":{},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"the device type is\", device)\n"]},{"cell_type":"markdown","id":"b405564a-0498-416c-810e-6639df27c759","metadata":{},"outputs":[],"source":["## Image Processing and Load Data for Dataset preparation\n"]},{"cell_type":"markdown","id":"8ba49195-2e4d-49ec-94bf-2c328ddbcf44","metadata":{},"outputs":[],"source":["In this section, we will preprocess our dataset by resizing the images, converting them to tensors, and normalizing the image channels. These are the standard preprocessing steps for image data. In addition, we will apply data augmentation to the training dataset to improve generalization. The preprocessing steps for the test (or validation) dataset are the same, except that data augmentation is not applied, as we want to evaluate the model on unmodified images.\n"]},{"cell_type":"markdown","id":"dd8d607f-28d8-4cac-a0d2-06e1718f6687","metadata":{},"outputs":[],"source":["**Mean and standard deviation values (used for normalization; based on ImageNet)**\n","\n","```python\n","mean = [0.485, 0.456, 0.406]\n","std = [0.229, 0.224, 0.225]\n","\n","# Define composed transform for training dataset\n","composed = transforms.Compose([\n","    transforms.Resize((224, 224)),              # Resize to match model input\n","    transforms.RandomHorizontalFlip(),          # Randomly flip images horizontally\n","    transforms.RandomRotation(degrees=5),       # Small random rotation\n","    transforms.ToTensor(),                      # Convert image to tensor [C, H, W]\n","    transforms.Normalize(mean, std)             # Normalize using predefined mean and std\n","])\n","```\n"]},{"cell_type":"markdown","id":"3d2c3b1e-0ac5-4017-bab1-defb67286cb6","metadata":{},"outputs":[],"source":["**Load Data for Dataset preparation**\n"]},{"cell_type":"markdown","id":"5f50542b-f123-4f82-a5f9-ae29d4e8c6af","metadata":{},"outputs":[],"source":["Download the data:\n"]},{"cell_type":"code","id":"0e805dd6-96fe-42ac-83ac-4ff667726d53","metadata":{},"outputs":[],"source":["# URL of the ZIP file\nurl = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/QkkP2jRxVvxHKMPg8lnkwQ/transfer-learning-with-cnn-15-2025-06-24-t-12-35-01-829-z.zip\"\n\n# Send a GET request to the URL\nresponse = requests.get(url)\n\n# Check if the request was successful\nif response.status_code == 200:\n    # Open the zip file from the downloaded content\n    with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:\n        zip_ref.extractall(\"hotdog_nothotdog\")  # Extract to a target folder\n    print(\"Download and extraction complete.\")\nelse:\n    print(\"Failed to download file:\", response.status_code)\n"]},{"cell_type":"markdown","id":"96bfa957-8854-431c-aa5a-0d51fabf9078","metadata":{},"outputs":[],"source":["Splits image data into training and validation sets\n"]},{"cell_type":"markdown","id":"18bf6998-6f8e-4e88-bc1c-88efa2553ef4","metadata":{},"outputs":[],"source":["Set Paths\n"]},{"cell_type":"code","id":"4c1e3293-59c2-4e34-b598-f383b85376ce","metadata":{},"outputs":[],"source":["#sets path\nsource_dir = \"hotdog_nothotdog/transfer-learning-with-cnn-15-2025-06-24-t-12-35-01-829-z\"  # folder containing images and annotation file\nannotations_file = os.path.join(source_dir, \"_annotations.json\")  # update name if needed"]},{"cell_type":"markdown","id":"0c984ac6-fe8d-41a9-aad3-c2ed14d30958","metadata":{},"outputs":[],"source":["Load annotations\n"]},{"cell_type":"code","id":"ae5f975a-33b1-4d67-9f4d-112862196c0a","metadata":{},"outputs":[],"source":["# Load annotations\nwith open(annotations_file, \"r\") as f:\n    annotations = json.load(f)"]},{"cell_type":"markdown","id":"661019a8-bdc7-4336-a075-71952dfcc649","metadata":{},"outputs":[],"source":["Set Parameters\n"]},{"cell_type":"code","id":"5918410f-a540-4eb5-a7a6-2260f3d0cc81","metadata":{},"outputs":[],"source":["# Parameters\ntrain_ratio = 0.9\noutput_dir = \"dataset\"  # Final output root directory\n"]},{"cell_type":"markdown","id":"3570ca68-2582-49b7-b4c4-50976ad4c525","metadata":{},"outputs":[],"source":["Prepare Label to Image Mapping\n"]},{"cell_type":"code","id":"7a6b2541-124c-44cd-b522-f48cd4032cc2","metadata":{},"outputs":[],"source":["# Prepare label -> image list\nlabel_to_images = {}\n\nfor filename, entry in annotations[\"annotations\"].items():\n    label = entry[0][\"label\"]\n    label_to_images.setdefault(label, []).append(filename)"]},{"cell_type":"markdown","id":"c67d932d-860b-42fc-b88d-2cb2336b7d56","metadata":{},"outputs":[],"source":["Shuffle and Split into Train/Validation\n","\n","**Splitting Dataset: 90% Training, 10% Validation**\n","We define `train_ratio = 0.9` and apply it to split each class. The first 90% of shuffled images are used for training, and the remaining 10% for validation.\n"]},{"cell_type":"code","id":"6ca8b66f-065d-47be-8395-fc495fb457cb","metadata":{},"outputs":[],"source":["# Shuffle and split each class into training and validation sets\nfor label, image_list in label_to_images.items():\n    random.shuffle(image_list)  # Shuffle the list of images to randomize the split\n    \n    # Calculate the number of training images (e.g., 90% of total)\n    train_cutoff = int(len(image_list) * train_ratio)\n    \n    # Split the image list into training and validation sets\n    train_images = image_list[:train_cutoff]\n    val_images = image_list[train_cutoff:]\n\n    # Loop over both splits: 'train' and 'val'\n    for split, split_images in zip([\"train\", \"val\"], [train_images, val_images]):\n        \n        # Create the output directory for the current split and label\n        # Example: dataset/train/hotdog or dataset/val/nothotdog\n        out_path = os.path.join(output_dir, split, label)\n        os.makedirs(out_path, exist_ok=True)  # Create the directory if it doesn't exist\n\n        # Copy each image from the source directory to the appropriate split folder\n        for img_name in split_images:\n            src = os.path.join(source_dir, img_name)  # Full path to the source image\n            dst = os.path.join(out_path, img_name)    # Destination path\n            shutil.copy2(src, dst)  # Copy the image (preserves metadata)\n\n# Print completion message once all images are copied\nprint(\"Train/Val split complete.\")\n"]},{"cell_type":"markdown","id":"d3ba95fe-9335-4431-bd11-9c0d6f0abfa6","metadata":{},"outputs":[],"source":["Apply transformation\n"]},{"cell_type":"code","id":"da651cfa-21d1-4d84-8220-2a5897447c86","metadata":{},"outputs":[],"source":["# Define a series of transformations to apply to each image\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize all images to 224x224 pixels (standard for pretrained models)\n    transforms.ToTensor(),          # Convert PIL Image to PyTorch tensor with shape [C, H, W] and values in [0, 1]\n    transforms.Normalize(           # Normalize the image using ImageNet's mean and std for each channel (RGB)\n        [0.485, 0.456, 0.406],      # Mean for Red, Green, Blue\n        [0.229, 0.224, 0.225]       # Standard deviation for Red, Green, Blue\n    )\n])\n\n# Load training dataset from folder structure and apply the defined transformations\ntrain_dataset = ImageFolder(\"dataset/train\", transform=transform)\n\n# Load validation dataset from folder structure and apply the same transformations\n# (Note: No augmentation is applied here — only resizing, tensor conversion, and normalization)\nval_dataset = ImageFolder(\"dataset/val\", transform=transform)"]},{"cell_type":"markdown","id":"82861ee2-84b7-4ab2-a06e-84d4f92ca147","metadata":{},"outputs":[],"source":["We can plot some of our dataset from Validation set:\n"]},{"cell_type":"code","id":"e1d64bb0-b87d-41f1-b44e-f87de2bf3ad4","metadata":{},"outputs":[],"source":["i = 0\nfor x, y in val_dataset:                     # Loop through validation dataset\n    imshow_(x, f\"y = {y}\")               # Display the image with its label\n    i += 1                               # Increment counter\n    if i == 3:                           # Stop after showing 3 images\n        break"]},{"cell_type":"markdown","id":"cdb3fab4-4cb8-4e20-8451-1ade977c1ea2","metadata":{},"outputs":[],"source":["### Hyperparameters\n"]},{"cell_type":"markdown","id":"befc8165-dde8-42e3-b3d7-057a380e20e9","metadata":{},"outputs":[],"source":["Experiment with different hyperparameters:\n"]},{"cell_type":"markdown","id":"448eb379-a429-41fc-9792-97e577734c04","metadata":{},"outputs":[],"source":["<b>Epoch</b> indicates the number of passes of the entire training dataset, here we will set the number of epochs to 10:\n"]},{"cell_type":"code","id":"d99ae071-725a-452d-a36e-4c534dea7fd7","metadata":{},"outputs":[],"source":["n_epochs=10"]},{"cell_type":"markdown","id":"20f95780-6e65-4f20-bbd7-81668e240845","metadata":{},"outputs":[],"source":["<b>Batch size</b> is the number of training samples utilized in one iteration. If the batch size is equal to the total number of samples in the training set, then every epoch has one iteration. In Stochastic Gradient Descent, the batch size is set to one. A batch size of 32--512 data points seems like a good value, for more information check out the following <a href=\"https://arxiv.org/abs/1609.04836?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-cvstudio-2021-01-01\">link</a>.\n"]},{"cell_type":"code","id":"0a2fde29-b7d4-47a9-9d80-181f01090a14","metadata":{},"outputs":[],"source":["batch_size=32"]},{"cell_type":"markdown","id":"3eb1ee24-c704-4695-8d8e-25806e07b5cf","metadata":{},"outputs":[],"source":[" <b>Learning rate</b> is used in the training of neural networks. Learning rate is a hyperparameter with a small positive value, often in the range between 0.0 and 1.0.\n"]},{"cell_type":"code","id":"d9223132-0748-44b3-b737-a0743881ba27","metadata":{},"outputs":[],"source":["lr=0.000001"]},{"cell_type":"markdown","id":"89d3de24-9603-4470-9b4a-66dfed1e82c3","metadata":{},"outputs":[],"source":["<b>Momentum</b> is an optimization technique used with gradient descent to accelerate convergence (when a models training process stabilizes and stops making significant improvements), reduce oscillations in steep valleys, and help escape local minimas.\n"]},{"cell_type":"code","id":"517a9d88-b9f3-4791-ba9d-e0d59dd04678","metadata":{},"outputs":[],"source":["momentum=0.9"]},{"cell_type":"markdown","id":"f1113273-45cb-44a2-b4f8-af1594c1afb1","metadata":{},"outputs":[],"source":["If you set to <code>lr_scheduler=True</code>  for every epoch use a learning rate scheduler changes the range of the learning rate from a maximum or minimum value. The learning rate usually decays over time.\n"]},{"cell_type":"code","id":"b5ac1ef3-a3aa-4d31-9d46-a4786997b4d6","metadata":{},"outputs":[],"source":["lr_scheduler=True\nbase_lr=0.001\nmax_lr=0.01"]},{"cell_type":"markdown","id":"4edb065c-9403-4c61-a57e-f141f20df71f","metadata":{},"outputs":[],"source":["# Load Model and Train\n"]},{"cell_type":"markdown","id":"e34d065b-fde4-4bb4-9b02-8a878f37c482","metadata":{},"outputs":[],"source":["The `train_model()` function will train the model:\n","\n","Steps Performed by `train_model()`\n","\n","- Initialize Tracking Variables\n","  \n","         Set up lists to store training loss and validation accuracy per epoch.\n","         Store initial model weights as the best model so far.  \n","  \n","- Loop Over Epochs\n","\n","         For each epoch (iteration over the full training dataset):\n","\n","**Training Phase:**\n","- Loop Over Batches in train_loader\n","\n","         Move input data (x) and labels (y) to the appropriate device (CPU or GPU).\n","         Set model to training mode.\n","         Perform a forward pass through the model.\n","         Calculate the loss using the criterion.\n","         Call backward() to compute gradients.\n","         Call optimizer.step() to update weights.\n","         Reset gradients with optimizer.zero_grad().\n","\n","- Calculate Average Training Loss for the Epoch\n","\n","         Append mean of batch losses to the loss_list.\n","\n","- Update Learning Rate (if Scheduler is Provided)\n","\n","         Call scheduler.step() to adjust learning rate.\n","\n","**Validation Phase:**\n","- Evaluate Model on Validation Set\n","\n","         Set model to evaluation mode.\n","         Disable gradient computation using torch.no_grad().\n","         Predict labels and compare to ground truth.\n","         Accumulate total correct predictions to compute accuracy.\n","\n","- Track and Save the Best Model\n","\n","         If current validation accuracy is better than the previous best, save model weights.\n","\n","- Print Epoch Metrics (Optional)\n","\n","         Print current learning rate, validation loss, and accuracy if print_ is True.\n","\n","- Load Best Model Weights\n","\n","         After all epochs, load the best-performing model weights (based on validation accuracy).\n","\n","- Return Results\n","\n","         Return the list of validation accuracies, training losses, and the trained model.\n"]},{"cell_type":"code","id":"0739105d-50e6-4d7c-babe-9bf0840ca5b8","metadata":{},"outputs":[],"source":["def train_model(model, train_loader, validation_loader, criterion, optimizer, n_epochs, print_=True):\n    loss_list = []        # Store average training loss per epoch\n    accuracy_list = []    # Store validation accuracy per epoch\n    correct = 0\n\n    n_test = len(val_dataset)  # Total number of validation samples\n    accuracy_best = 0      # Track best validation accuracy\n    best_model_wts = copy.deepcopy(model.state_dict())  # Backup best model weights\n\n    print(\"The first epoch should take several minutes\")\n\n    for epoch in tqdm(range(n_epochs)):  # Loop through each epoch\n        loss_sublist = []  # Store individual batch losses for this epoch\n\n        # Training phase\n        for x, y in train_loader:\n            x, y = x.to(device), y.to(device)\n            model.train()  # Set model to training mode\n\n            z = model(x)   # Forward pass\n            loss = criterion(z, y)  # Compute loss\n            loss_sublist.append(loss.item())\n\n            loss.backward()       # Backpropagation\n            optimizer.step()      # Update weights\n            optimizer.zero_grad() # Reset gradients\n\n        print(f\"Epoch {epoch + 1} done\")\n\n        # Adjust learning rate if scheduler is defined\n        scheduler.step()\n\n        # Store average training loss for this epoch\n        loss_list.append(np.mean(loss_sublist))\n\n        # Validation phase\n        correct = 0\n        model.eval()  # Set model to evaluation mode\n        with torch.no_grad():\n            for x_test, y_test in validation_loader:\n                x_test, y_test = x_test.to(device), y_test.to(device)\n                z = model(x_test)\n                _, yhat = torch.max(z.data, 1)\n                correct += (yhat == y_test).sum().item()\n\n        accuracy = correct / n_test\n        accuracy_list.append(accuracy)\n\n        # Save best model\n        if accuracy > accuracy_best:\n            accuracy_best = accuracy\n            best_model_wts = copy.deepcopy(model.state_dict())\n\n        # Print training progress\n        if print_:\n            print(\"Learning rate:\", optimizer.param_groups[0]['lr'])\n            print(f\"Validation loss (epoch {epoch + 1}): {np.mean(loss_sublist):.4f}\")\n            print(f\"Validation accuracy (epoch {epoch + 1}): {accuracy:.4f}\")\n\n    # Load best model weights before returning\n    model.load_state_dict(best_model_wts)\n    return accuracy_list, loss_list, model"]},{"cell_type":"markdown","id":"ff5c5187-06f2-4708-8aa9-21539952dcd9","metadata":{},"outputs":[],"source":[" Load the pre-trained model resnet18. Set the parameter pretrained to true.\n"]},{"cell_type":"code","id":"fb86234b-a955-49e5-a49d-53a2aac45b7f","metadata":{},"outputs":[],"source":["model = models.resnet18(pretrained=True)"]},{"cell_type":"markdown","id":"59b832b2-3be2-4f62-abd8-0d5c84ef66be","metadata":{},"outputs":[],"source":["We will only train the last layer of the network set the parameter <code>requires_grad</code> to <code>False</code>, the network is a fixed feature extractor.\n"]},{"cell_type":"code","id":"1865ca3b-271f-4511-8cb0-5570276a951c","metadata":{},"outputs":[],"source":["for param in model.parameters():\n        param.requires_grad = False\n    "]},{"cell_type":"markdown","id":"6bc1b9a5-57f8-4c21-a3f3-c56560fce34a","metadata":{},"outputs":[],"source":["Number of classes\n"]},{"cell_type":"code","id":"1e8bf942-df40-40ad-9058-9487f38d6da9","metadata":{},"outputs":[],"source":["n_classes = len(train_dataset.classes)\nprint(n_classes)\n"]},{"cell_type":"markdown","id":"5b105b6a-f1bf-4983-805f-321165cb93d9","metadata":{},"outputs":[],"source":["Replace the output layer model.fc of the neural network with a nn.Linear object, to classify <code>n_classes</code> different classes. For the parameters in_features  remember the last hidden layer has 512 neurons.\n"]},{"cell_type":"code","id":"b0c6067b-7497-4781-8fa5-84061800fd66","metadata":{},"outputs":[],"source":["model.fc = nn.Linear(512, n_classes)"]},{"cell_type":"markdown","id":"3f962bb5-0487-4f9e-830a-6ee5632be785","metadata":{},"outputs":[],"source":["Set device type\n"]},{"cell_type":"code","id":"f2bae2bc-873e-49d2-b9eb-3cf794e17ca4","metadata":{},"outputs":[],"source":["model.to(device)"]},{"cell_type":"markdown","id":"696d705f-ee9d-4df7-af47-d6872b627c7e","metadata":{},"outputs":[],"source":["Cross-entropy loss, or log loss, measures the performance of a classification model combines LogSoftmax in one object class. It is useful when training a classification problem with C classes.\n"]},{"cell_type":"code","id":"7ce88267-b32f-46b6-b74c-fafac993b7de","metadata":{},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()"]},{"cell_type":"markdown","id":"72d0b98d-eb82-4e22-8cfc-12c11615e6d6","metadata":{},"outputs":[],"source":["Create a training loader and validation loader object.\n"]},{"cell_type":"code","id":"63fc83b6-d405-4a9d-8e1c-75a02044d382","metadata":{},"outputs":[],"source":["train_loader = torch.utils.data.DataLoader(dataset=train_dataset , batch_size=batch_size,shuffle=True)\nvalidation_loader= torch.utils.data.DataLoader(dataset=val_dataset , batch_size=1)"]},{"cell_type":"markdown","id":"7edcc0cd-3bc8-4f04-935b-771d6a17126b","metadata":{},"outputs":[],"source":["Use the optim package to define an Optimizer that will update the weights of the model for us. \n"]},{"cell_type":"code","id":"3855bbe2-9f5c-4d52-b554-564ca1bbd373","metadata":{},"outputs":[],"source":["optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)"]},{"cell_type":"markdown","id":"68b813b8-a95d-471b-bfdb-fc50b4635bcb","metadata":{},"outputs":[],"source":["We use <a href='https://arxiv.org/pdf/1506.01186.pdf'>Cyclical Learning Rates</a>\n"]},{"cell_type":"code","id":"8186a11a-b081-4047-876d-8b451d3a9c20","metadata":{},"outputs":[],"source":["if lr_scheduler:\n    scheduler = torch.optim.lr_scheduler.CyclicLR(\n        optimizer,\n        base_lr=0.001,        # Minimum learning rate\n        max_lr=0.01,          # Maximum learning rate\n        step_size_up=5,       # Steps to increase LR from base_lr to max_lr\n        mode=\"triangular2\"    # Learning rate follows triangular cycle and halves the max_lr each cycle\n    )\n"]},{"cell_type":"markdown","id":"7d5948ee-b2d2-47b0-97a0-4bb19161449f","metadata":{},"outputs":[],"source":["Now we are going to train model,for the given images this take 25 minutes, depending on your dataset\n"]},{"cell_type":"code","id":"46e91aad-d4ab-40a4-b4b9-3efb25de2ec5","metadata":{},"outputs":[],"source":["# Start time tracking\nstart_datetime = datetime.now()\nstart_time = time.time()\n\n# Train the model\naccuracy_list, loss_list, model = train_model(\n    model, train_loader, validation_loader, criterion, optimizer, n_epochs=n_epochs\n)\n\n# End time tracking\nend_datetime = datetime.now()\nelapsed_time = time.time() - start_time\n\n# Print results\nprint(\"Training completed.\")\nprint(f\"Start Time     : {start_datetime.strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(f\"End Time       : {end_datetime.strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(f\"Elapsed Time   : {elapsed_time:.2f} seconds\")\n"]},{"cell_type":"markdown","id":"fde76117-bb30-413d-be03-bb0580e1a5d5","metadata":{},"outputs":[],"source":["Save the model to model.pt\n"]},{"cell_type":"code","id":"004c5f57-bc4f-4fad-9ea4-94f6e9ad1208","metadata":{},"outputs":[],"source":["# Save the model to model.pt\ntorch.save(model.state_dict(), 'model.pt')"]},{"cell_type":"markdown","id":"26a1ee04-fbd8-475c-9212-92fdbe7aeeab","metadata":{},"outputs":[],"source":["Plot train cost and validation accuracy,  you can improve results by getting more data.\n"]},{"cell_type":"code","id":"369b96c4-1feb-4710-9f1a-8583c42250bc","metadata":{},"outputs":[],"source":["plot_stuff(loss_list,accuracy_list)"]},{"cell_type":"markdown","id":"50aff092-2e5a-4961-9817-22d31bfbbeff","metadata":{},"outputs":[],"source":["# Practice Exercise \n","\n","### Test Our Model with an Uploaded Image\n"]},{"cell_type":"markdown","id":"ba245264-8304-4f48-9cbb-991f74bfa587","metadata":{},"outputs":[],"source":["Upload your image, and see if it will be correctly classified.\n","<p><b>Instructions on How to Upload an Image:</b></p>\n","Use the upload button and upload an image from your local machine:\n","<center>\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images/instruction.png\" width=\"300\"  />\n","</center>\n"]},{"cell_type":"markdown","id":"97ba84f4-9fbd-4c0f-b07b-a9036fb01c57","metadata":{},"outputs":[],"source":["The image will now be in the directory in which you are working in. To read the image in a new cell, use the <code>cv2.imread</code> and read its name. For example, I uploaded <code>anothercar.jpg</code> into my current working directory - <code>cv2.imread(\"anothercar.jpg\")</code>.\n","\n","<center>\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images/instruction2.png\" width=\"300\"  />\n","</center>\n"]},{"cell_type":"markdown","id":"5134737e-940d-4b93-a6d1-bee3b052ba4d","metadata":{},"outputs":[],"source":["Else use the below images to test.\n"]},{"cell_type":"code","id":"13a43b12-0dc4-4d51-aac2-f2c69a8562fc","metadata":{},"outputs":[],"source":["!wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/eTwtaXXHQQkxDlkFWHqRNw/test.jpg\"\n!wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/xGq5q1hvl7HQLhmVGAOfzQ/test1.jpg\"\n"]},{"cell_type":"markdown","id":"c4c4eb8d-4df9-41d7-9327-417e248c77ef","metadata":{},"outputs":[],"source":["Define the class name and Load the model\n"]},{"cell_type":"code","id":"9192fe78-3c16-4bb1-8774-8ad2f73d9b4a","metadata":{},"outputs":[],"source":["# Define class names (as per training)\nclass_names = ['hotdog', 'nothotdog']\n# Create the same model architecture as during training\nmodel = models.resnet18(pretrained=False)\nmodel.fc = torch.nn.Linear(model.fc.in_features, 2)  # 2 classes: hotdog / nothotdog\n\n# Load trained weights\nmodel.load_state_dict(torch.load(\"model.pt\", map_location=torch.device('cpu')))\nmodel.eval()  # Set to evaluation mode\n"]},{"cell_type":"markdown","id":"4c48c421-6d26-463c-8ca2-c5b1f72c6fd9","metadata":{},"outputs":[],"source":["Define Image Transformation\n"]},{"cell_type":"code","id":"8205be17-e16e-4b39-a310-b776320e843f","metadata":{},"outputs":[],"source":["#Define image transformations (must match training)\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize image\n    transforms.ToTensor(),  # Convert to tensor\n    transforms.Normalize([0.485, 0.456, 0.406],  # Normalize (same as ImageNet/pretrained)\n                         [0.229, 0.224, 0.225])\n])\n"]},{"cell_type":"markdown","id":"1d55a21e-19fa-4c20-8edd-4ad5269525af","metadata":{},"outputs":[],"source":["Load and Preprocess the Image\n","\n","Replace your_uploaded_file below with the name of your image as seen in your directory. In case you are using the downloaded images given in the notebook then use as `test.jpg` or `test1.jpg`\n"]},{"cell_type":"code","id":"d27500ff-4457-4bfe-8d72-c5387b78010e","metadata":{},"outputs":[],"source":["image_path = \"your_uploaded_file.jpg\"  # Replace with your image path\n\n# Open and convert to RGB\nimage = Image.open(image_path).convert(\"RGB\")\n\n# Apply transformations\ninput_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n"]},{"cell_type":"markdown","id":"28891b12-1d23-41c8-9ecd-abaa67a96694","metadata":{},"outputs":[],"source":["Make Prediction and show result\n"]},{"cell_type":"code","id":"4f6bfd8c-dfaf-4af3-8a4b-2ab1b06be216","metadata":{},"outputs":[],"source":["with torch.no_grad():\n    outputs = model(input_tensor)\n    predicted_class = torch.argmax(outputs, 1).item()\n#Display result\nprint(f\"The image was classified as: {class_names[predicted_class]}\")\n# Display the image with predicted label\nplt.imshow(image)  # Original PIL image\nplt.title(f\"Predicted: {class_names[predicted_class]}\")\nplt.axis(\"off\")\nplt.show()"]},{"cell_type":"markdown","id":"3a9f4e23-1ea1-418c-bbbc-339e95d70e8b","metadata":{},"outputs":[],"source":["### Congratulations! You've completed the image classification lab using transfer learning. \n","You successfully leveraged a pretrained deep neural network to build an effective image classifier, experimented with hyperparameters, and gained practical experience applying transfer learning—a widely used approach in modern computer vision.\n"]},{"cell_type":"markdown","id":"00b59e4c-9e5a-4a0d-82d3-9c77216448ae","metadata":{},"outputs":[],"source":["## Authors\n"]},{"cell_type":"markdown","id":"06c391a2-d40b-4787-a7ee-eda1edb4bf8f","metadata":{},"outputs":[],"source":["Joseph Santarcangelo\n","\n","[Sathya Priya](https://www.linkedin.com/in/sathya-priya-06120a17a/) \n","\n"]},{"cell_type":"markdown","id":"8a2f61a1-caeb-4f1b-b257-4062853ef8f9","metadata":{},"outputs":[],"source":["<!--## Change Log-->\n"]},{"cell_type":"markdown","id":"f150f2b5-768d-4ced-8d1c-7ed3a0751500","metadata":{},"outputs":[],"source":["<!--| Date (YYYY-MM-DD) | Version | Changed By | Change Description      |\n","| ----------------- | ------- | ---------- | ----------------------- |\n","| 2025-06-25        | 0.4    | Sathya Priya| Created and Converted the lab to JupyterCurrent notebook |\n","| 2021-05-25        | 0.3     | Yasmine    | Modifies Multiple Areas |\n","| 2021-05-25        | 0.3     | Kathy      | Modified Multple Areas. |\n","| 2021-03-08        | 0.2     | Joseph     | Modified Multiple Areas |\n","| 2021-02-01        | 0.1     | Joseph     | Modified Multiple Areas |-->\n"]},{"cell_type":"markdown","id":"ff9131a9-e616-48b0-ae3d-4338aa045e74","metadata":{},"outputs":[],"source":["<h3 align=\"center\"> &#169; IBM Corporation. All rights reserved. <h3/>\n"]},{"cell_type":"code","id":"e3719f59-033d-489a-9a6a-289a3a180396","metadata":{},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.8"},"prev_pub_hash":"33774c331d7ec8d1a4023d8695eb13a2f58b6b7c645a25d26354355c80963188"},"nbformat":4,"nbformat_minor":4}