{"cells":[{"cell_type":"markdown","id":"8ae2f90f-a80f-4ce8-8408-4b123d53e1dd","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","id":"77134af1-eba1-45cb-a737-3f4c0bfff23d","metadata":{},"outputs":[],"source":["<h1>Final Project: Building an Image Classifier with Transfer Learning</h1>\n"]},{"cell_type":"markdown","id":"52bbb8dd-b792-4876-958d-d50085174547","metadata":{},"outputs":[],"source":["Estimated time needed: **60** minutes\n"]},{"cell_type":"markdown","id":"6b7311b5-e955-4434-90f8-1eb3f89e4b88","metadata":{},"outputs":[],"source":["## Overview\n"]},{"cell_type":"markdown","id":"59687e3f-4ad9-4f6d-833f-7444f5407344","metadata":{},"outputs":[],"source":["In this lab, you will implement image classification using using <a href=\"https://cs231n.github.io/transfer-learning/\">transfer learning</a>,with a pre-trained deep neural network. You will work with a provided image dataset and experiment with various hyperparameters to optimize model performance.\"\n"]},{"cell_type":"markdown","id":"7cc2f190-e5a9-4fc7-8dfe-fe4addbbb644","metadata":{},"outputs":[],"source":["## Objectives\n"]},{"cell_type":"markdown","id":"d16982d1-7594-4c50-847a-014db0575cb1","metadata":{},"outputs":[],"source":["Youâ€™ll build an effective image classifier using a pretrained Convolutional Neural Network (ConvNet). In practice, very few people train an entire ConvNet from scratch with random initialization, as this requires a very large labeled dataset and significant computational resources. Instead, it is common to use a ConvNet that has been pretrained on a large dataset (such as ImageNet) by others, and adapt it for a specific taskâ€”a process known as transfer learning.\n","Here, weâ€™ll use the pretrained network as a feature extractor by training only the final output layer on your dataset. Typically, 100â€“200 images are enough to get started, and training can be completed in a short amount of time. While adding more images can improve performance, the gains often diminish as the dataset grows.\n"]},{"cell_type":"markdown","id":"681e7973-3006-434b-8a1d-b9a1957a4241","metadata":{},"outputs":[],"source":["## Instruction(s)\n","After completing the Notebook:\n","* Take the required screenshots of your work.\n","* Download the notebook using **File** > **Download**.\n","* You can either choose:\n","  \n","   **Option 1 â€“ AI-Graded Submission:** Upload the downloaded notebook directly for AI-based evaluation.\n","\n","   **Option 2 â€“ Peer-Reviewed Submission:** Submit the screenshots you captured as evidence of your completed work                                                for peer evaluation.\n"]},{"cell_type":"markdown","id":"4415c39f-7741-4a56-b172-3dbae1909a3d","metadata":{},"outputs":[],"source":["# About the Dataset\n","\n","The dataset used in this project consists of labeled images for two categories:\n","\n","* **\"Stop\" Sign Images** â€“ pictures that contain stop signs\n","* **\"Not Stop\" Sign Images** â€“ pictures that do not contain any stop sign\n","\n","These images have been collected and manually labeled to support supervised learning for a binary image classification task. The goal is to train a model that can distinguish between images that contain stop signs and those that do not.\n","\n","ðŸ”— **Download Links:**\n","\n","<a href=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/ExisQFol3hUHktTjm6a51w/final-project-stop-signs-1-2025-04-25-t-06-47-41-058-z.zip\" target=\"_blank\">\n","    Download Dataset\n","</a>\n","\n","These ZIP files contain images grouped and are to be extracted into appropriate folders for training and validation.\n","\n","## Dataset Structure After Extraction\n","\n","```\n","/not_stopandstop\n","â”œâ”€â”€ final-project-stop-signs-1-2025-04-25-t-06-47-41-058-z\n","    â”œâ”€â”€ 001.jpg\n","    â”œâ”€â”€ 002.jpg\n","    â””â”€â”€ ...\n","```\n","\n","## Dataset Summary\n","\n","| Attribute    | Description                                            |\n","| ------------ | ------------------------------------------------------ |\n","| Image Format | JPG                                           |\n","| Image Size   | Varies (to be resized during preprocessing)            |\n","| Labels       | Binary: `not_stop`, `stop`                             |\n","| Task Type    | Image Classification                                   |\n","| Use Case     | Training a model to identify stop signs in road scenes |\n","\n","## Usage Instructions\n","\n","* Resize images to 224Ã—224 pixels to ensure consistent input dimensions.\n","* Normalize image channels using mean [0.485, 0.456, 0.406] and std [0.229, 0.224, 0.225].\n","* Split the dataset into training and validation sets (e.g., 90/10 ratio).\n","* Apply data augmentation like random flips and small rotations on training data.\n","* Train a classifier using CNN or a pretrained model (e.g., ResNet).\n","* Evaluate the model using accuracy on the validation set.                                                       \n","\n","This dataset simulates a real-world autonomous driving task and is ideal for practicing image classification pipelines from data preprocessing to model evaluation.\n","\n"]},{"cell_type":"markdown","id":"eea8d934-4841-4e42-8f9f-68e799a39180","metadata":{},"outputs":[],"source":["## Install and Import Libraries\n"]},{"cell_type":"markdown","id":"88bf226f-1ce1-49cf-8e15-703e4a636494","metadata":{},"outputs":[],"source":["**May take time for installation so please wait...**\n"]},{"cell_type":"code","id":"47c98cc1-63c6-40b8-9c64-dc435ced785b","metadata":{},"outputs":[],"source":["!pip install torch torchvision\n!pip install numpy pandas matplotlib tqdm pillow --quiet\n"]},{"cell_type":"markdown","id":"a285a036-8f00-484a-be91-f0e19285ad0b","metadata":{},"outputs":[],"source":["Libraries for OS and Cloud\n"]},{"cell_type":"code","id":"67d8d716-9a93-4874-8fb9-65387ec14b4c","metadata":{},"outputs":[],"source":["import os\nimport uuid\nimport shutil\nimport json\nimport copy\nfrom datetime import datetime\nimport zipfile\nimport io\nimport requests\nimport random"]},{"cell_type":"markdown","id":"4f7f1eae-e75c-475e-a953-358b16be6682","metadata":{},"outputs":[],"source":["Libraries for Data Processing and Visualization\n"]},{"cell_type":"code","id":"f0055149-1126-4704-8232-c4d11585547a","metadata":{},"outputs":[],"source":["from PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport math\nfrom matplotlib.pyplot import imshow\nfrom tqdm import tqdm\nfrom ipywidgets import IntProgress\nimport time "]},{"cell_type":"markdown","id":"796db94b-c7a9-46c1-aaca-1b839fd4c46c","metadata":{},"outputs":[],"source":["Deep Learning Libraries\n"]},{"cell_type":"code","id":"720fbdea-2992-429a-9455-6d454497b4ac","metadata":{},"outputs":[],"source":["import torch\nimport torchvision.models as models\nfrom torch.utils.data import Dataset, DataLoader,random_split\nfrom torch.optim import lr_scheduler\nfrom torchvision import transforms\nimport torch.nn as nn\ntorch.manual_seed(0)\nfrom torchvision.datasets import ImageFolder\nfrom torchvision import transforms"]},{"cell_type":"markdown","id":"224ac11d-0b70-41c2-8650-1528e890c351","metadata":{},"outputs":[],"source":["**Plot train cost and validation accuracy:**\n","\n","The `plot_stuff` function is used to visualize the model's training performance by plotting training loss and validation accuracy on the same graph using two different y-axes. \n","\n","The training loss, shown in red on the left y-axis, indicates how well the model is minimizing error during training. \n","\n","The validation accuracy, shown in blue on the right y-axis, reflects how well the model is performing on unseen data. \n","\n","By plotting both metrics together, this function helps monitor the learning process, identify trends, and detect issues such as overfittingâ€”where the model performs well on training data but poorly on validation data. \n","This kind of visualization is valuable for understanding the effectiveness of your training strategy and making informed adjustments to model architecture or hyperparameters.\n"]},{"cell_type":"code","id":"d45b4717-6ce2-4cf0-bf9e-69c0b2bfbef5","metadata":{},"outputs":[],"source":["def plot_stuff(COST, ACC):\n    \"\"\"\n    Plots training cost (loss) and validation accuracy on the same figure using two y-axes.\n    \n    Parameters:\n    COST (list or array): Total training loss per iteration (or epoch)\n    ACC (list or array): Validation accuracy per iteration (or epoch)\n    \"\"\"\n    \n    # Create a new figure and a primary axis (ax1)\n    fig, ax1 = plt.subplots()\n    \n    # Plot training loss on the primary y-axis (left)\n    color = 'tab:red'\n    ax1.plot(COST, color=color)\n    ax1.set_xlabel('Iteration', color=color)            # Label for x-axis\n    ax1.set_ylabel('Total Loss', color=color)           # Label for y-axis (left)\n    ax1.tick_params(axis='y', labelcolor=color)         # Set y-axis tick color\n\n    # Create a secondary y-axis (ax2) sharing the same x-axis\n    ax2 = ax1.twinx()\n    \n    # Plot validation accuracy on the secondary y-axis (right)\n    color = 'tab:blue'\n    ax2.set_ylabel('Accuracy', color=color)             # Label for y-axis (right)\n    ax2.plot(ACC, color=color)\n    ax2.tick_params(axis='y', labelcolor=color)\n\n    # Adjust layout to prevent y-label clipping\n    fig.tight_layout()\n    \n    # Display the combined plot\n    plt.show()\n"]},{"cell_type":"markdown","id":"a0368f62-b7f6-4664-b0f4-c419361770c1","metadata":{},"outputs":[],"source":["**Plot the transformed image:**\n","\n","When using `transforms.Normalize` during preprocessing (e.g., for pretrained CNNs like ResNet), images are normalized with mean and standard deviation values. But this makes the image look odd if displayed directly.\n","\n","This function reverses that normalization so the image appears as a proper RGB image for visualization, especially useful for:\n","\n","Checking your image pipeline.\n","\n","Displaying samples from datasets.\n","\n","Debugging predictions or model outputs.\n"]},{"cell_type":"code","id":"54a5df93-e9da-4e4b-a961-7c59d0c227d0","metadata":{},"outputs":[],"source":["def imshow_(inp, title=None):\n    \"\"\"\n    Displays a tensor image after reversing normalization.\n    \n    Parameters:\n    - inp (Tensor): Image tensor of shape [C, H, W], usually normalized.\n    - title (str, optional): Title for the image display.\n    \"\"\"\n    # Convert from [C, H, W] to [H, W, C] and to NumPy array\n    inp = inp.permute(1, 2, 0).numpy()\n    print(\"Image shape:\", inp.shape)\n\n    # Undo normalization (ImageNet mean and std)\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n\n    # Clip values to [0, 1] range for display\n    inp = np.clip(inp, 0, 1)\n\n    # Display image\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # Short pause for GUI update\n    plt.show()\n"]},{"cell_type":"markdown","id":"0e967fb6-40f8-4954-8471-1e3e5145747e","metadata":{},"outputs":[],"source":["Define our device as the first visible cuda device if we have CUDA available:\n"]},{"cell_type":"code","id":"b4e9627d-010b-4d30-8732-47dce980c11a","metadata":{},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"the device type is\", device)\n"]},{"cell_type":"markdown","id":"c3a161ed-2537-496c-952e-40813195684e","metadata":{},"outputs":[],"source":["## Image Processing and Load Data for Dataset preparationÂ¶\n"]},{"cell_type":"markdown","id":"0c1652ff-1963-4400-9fea-728c5b04ddb6","metadata":{},"outputs":[],"source":["In this section, we will preprocess our dataset by resizing the images, converting them to tensors, and normalizing the image channels. These are the standard preprocessing steps for image data. In addition, we will apply data augmentation to the training dataset to improve generalization. The preprocessing steps for the test (or validation) dataset are the same, except that data augmentation is not applied, as we want to evaluate the model on unmodified images.\n"]},{"cell_type":"markdown","id":"e5c7cace-476d-4f57-97ed-caf6f0ac7f97","metadata":{},"outputs":[],"source":["**Load Data for Dataset preparation**\n"]},{"cell_type":"markdown","id":"2ef9548f-f030-4995-8954-0ecac1e7490c","metadata":{},"outputs":[],"source":["Download the data:\n"]},{"cell_type":"code","id":"4e77ba72-2f7f-482b-8e98-bd13792b620d","metadata":{},"outputs":[],"source":["# URL of the ZIP file\nurl = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/ExisQFol3hUHktTjm6a51w/final-project-stop-signs-1-2025-04-25-t-06-47-41-058-z.zip\"\n\n# Send a GET request to the URL\nresponse = requests.get(url)\n\n# Check if the request was successful\nif response.status_code == 200:\n    # Open the zip file from the downloaded content\n    with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:\n        zip_ref.extractall(\"not_stopandstop\")  # Extract to a target folder\n    print(\"Download and extraction complete.\")\nelse:\n    print(\"Failed to download file:\", response.status_code)\n"]},{"cell_type":"markdown","id":"649a1daf-e892-44f9-a852-075c4abe6f5c","metadata":{},"outputs":[],"source":["Splits image data into training and validation sets: \n","90% of the data will be used for training.\n"]},{"cell_type":"markdown","id":"308cb481-7654-4dd3-9652-255cfa028db2","metadata":{},"outputs":[],"source":["Set Path\n"]},{"cell_type":"code","id":"83e25110-e51d-4844-90ad-e47351b23bc2","metadata":{},"outputs":[],"source":["#sets path\nsource_dir = \"not_stopandstop/final-project-stop-signs-1-2025-04-25-t-06-47-41-058-z\"  # folder containing images and annotation file\nannotations_file = os.path.join(source_dir, \"_annotations.json\")  # update name if needed"]},{"cell_type":"markdown","id":"684c4e2c-7472-478b-a912-c405269fff3b","metadata":{},"outputs":[],"source":["Load Annotations\n"]},{"cell_type":"code","id":"ebab5e5a-231a-4d1e-86be-e14f4e8b1c31","metadata":{},"outputs":[],"source":["# Load annotations\nwith open(annotations_file, \"r\") as f:\n    annotations = json.load(f)"]},{"cell_type":"markdown","id":"961b740f-83d1-4a34-9ead-c382b68f9e25","metadata":{},"outputs":[],"source":["Set Parameters\n"]},{"cell_type":"code","id":"4d9191a8-361e-495b-882d-b894a96c1fa3","metadata":{},"outputs":[],"source":["# Parameters\ntrain_ratio = 0.9\noutput_dir = \"dataset\"  # Final output root directory\n"]},{"cell_type":"markdown","id":"16541444-f0b0-4bf6-b58a-968152111a35","metadata":{},"outputs":[],"source":["Prepare Label to Image Mapping\n"]},{"cell_type":"code","id":"e75b657d-5539-4b1c-85dc-339a54166f4e","metadata":{},"outputs":[],"source":["# Prepare label -> image list\nlabel_to_images = {}\n\nfor filename, entry in annotations[\"annotations\"].items():\n    label = entry[0][\"label\"]\n    label_to_images.setdefault(label, []).append(filename)"]},{"cell_type":"markdown","id":"db65c20a-1d5d-48bf-8505-e1fce706b2a4","metadata":{},"outputs":[],"source":["Shuffle and Split into Train/Validation\n"]},{"cell_type":"code","id":"4b0fe894-b83f-46cf-b88d-3e9aae05cf0a","metadata":{},"outputs":[],"source":["# Shuffle and split each class into training and validation sets\nfor label, image_list in label_to_images.items():\n    random.shuffle(image_list)  # Shuffle the list of images to randomize the split\n    \n    # Calculate the number of training images (e.g., 90% of total)\n    train_cutoff = int(len(image_list) * train_ratio)\n    \n    # Split the image list into training and validation sets\n    train_images = image_list[:train_cutoff]\n    val_images = image_list[train_cutoff:]\n\n    # Loop over both splits: 'train' and 'val'\n    for split, split_images in zip([\"train\", \"val\"], [train_images, val_images]):\n        \n        # Create the output directory for the current split and label\n        # Example: dataset/train/not_stop or dataset/val/stop\n        out_path = os.path.join(output_dir, split, label)\n        os.makedirs(out_path, exist_ok=True)  # Create the directory if it doesn't exist\n\n        # Copy each image from the source directory to the appropriate split folder\n        for img_name in split_images:\n            src = os.path.join(source_dir, img_name)  # Full path to the source image\n            dst = os.path.join(out_path, img_name)    # Destination path\n            shutil.copy2(src, dst)  # Copy the image (preserves metadata)\n\n# Print completion message once all images are copied\nprint(\"Train/Val split complete.\")\n"]},{"cell_type":"markdown","id":"fd074d49-183e-4c6a-9637-af11d96cdcef","metadata":{},"outputs":[],"source":["Splitting Dataset: 90% Training, 10% Validation We define train_ratio = 0.9 and apply it to split each class. The first 90% of shuffled images are used for training, and the remaining 10% for validation.\n"]},{"cell_type":"markdown","id":"40cf5eeb-e334-4813-844c-9ea96e2da9f2","metadata":{},"outputs":[],"source":["## Exercise 1: Apply Image Transformations\n"]},{"cell_type":"markdown","id":"0bc84d3d-076a-45cd-892f-366e62681c8c","metadata":{},"outputs":[],"source":["Complete the following code to define a preprocessing pipeline that resizes, converts, and normalizes the images using ImageNet statistics.\n","```python\n","#Define a series of transformations to apply to each image\n","transform = transforms.Compose([\n","    transforms.Resize((___, ___)),  # Resize images to 224Ã—224 pixels to ensure consistent input dimensions.\n","    transforms.____(),              # Convert image to PyTorch tensor\n","    transforms.Normalize(\n","        mean=[0.485, 0.456, 0.406],     # Mean for ImageNet pretrained models\n","        std=[0.229, 0.224, 0.225]       # Std deviation for ImageNet pretrained models\n","    )\n","])\n","```\n"]},{"cell_type":"code","id":"b9dacc14-5699-4415-bedc-08473baea330","metadata":{},"outputs":[],"source":["# Write your response."]},{"cell_type":"markdown","id":"19f57d86-7607-4542-9424-74f4ae413b31","metadata":{},"outputs":[],"source":["**Note**: These normalization values center the data similarly to how the pretrained models were originally trained, which helps improve performance and convergence.\n","\n"]},{"cell_type":"markdown","id":"1153d557-a074-4f4e-a75f-1da808fd4063","metadata":{},"outputs":[],"source":["## Exercise 2: Load Datasets with Transformations\n","Complete the code to load the training and validation datasets using the correct folder paths and transformations.\n","\n","```python\n","\n","from torchvision.datasets import ImageFolder\n","\n","# Load datasets from the respective folders\n","train_dataset = ImageFolder(root=\"dataset/____\", transform=transform)\n","val_dataset = ImageFolder(root=\"dataset/____\", transform=transform)\n","```\n"]},{"cell_type":"code","id":"357b0f08-4022-4544-ad21-2ceb1e372bf8","metadata":{},"outputs":[],"source":["# Write your response."]},{"cell_type":"markdown","id":"21fc2498-716e-4488-b091-5d3c5fb59238","metadata":{},"outputs":[],"source":["**Note**: No augmentation is applied to the validation set â€” we want to evaluate model performance on real, unmodified images.\n"]},{"cell_type":"markdown","id":"4137b9cc-6622-4ac2-9989-72cbfffcef92","metadata":{},"outputs":[],"source":["## Exercise 3: Visualize Samples from the Validation Set\n"]},{"cell_type":"markdown","id":"0a8313d9-7174-48eb-894f-7c94006d7f0c","metadata":{},"outputs":[],"source":["Itâ€™s important to verify that image transformations are applied correctly and labels are loaded as expected.\n","\n","Complete the following code to display the first 3 images from the validation dataset along with their labels:\n","\n","```python\n","\n","# Loop through the validation dataset and display the first 3 images\n","# Loop through the dataset (x: image tensor, y: label)\n","# Display the image with its label using a custom display function\n","# Stop after showing 3 images\n","# Increment the counter\n","i = 0\n","for x, y in val_dataset:\n","    ____ (x, f\"y = {y}\")\n","    i += 1               \n","    if i == ___:        \n","        break\n","```\n"]},{"cell_type":"code","id":"d58c6c8c-f4ed-464d-a49b-8c2506041815","metadata":{},"outputs":[],"source":["# Write your response."]},{"cell_type":"markdown","id":"6f03d4c7-2d49-4f91-a1c4-6290e56570c6","metadata":{},"outputs":[],"source":["**Note**:Early visual inspection helps catch issues in image format, normalization, or label mismatch before training the model.\n"]},{"cell_type":"markdown","id":"f115427c-69e3-4098-8176-60e91e5c5d4d","metadata":{},"outputs":[],"source":["## Exercise 4: Reflect on Your Dataset and Transformations\n","\n","After applying transformations and visualizing sample images, consider the following questions:\n","\n","1. Why is normalization important when using pretrained models?\n","\n","2. What benefits does resizing the images to (224, 224) provide?\n","\n","3. What would happen if input sizes varied?\n"]},{"cell_type":"code","id":"a7b48bb6-b233-452c-9a67-174812f55661","metadata":{},"outputs":[],"source":["## Write your response here and convert the cell to a markdown."]},{"cell_type":"markdown","id":"456cab67-2f5f-4128-902c-6084a0e122d0","metadata":{},"outputs":[],"source":["## Hyperparameters\n"]},{"cell_type":"markdown","id":"5e42b751-19c4-4093-bd69-1e083b85b9d1","metadata":{},"outputs":[],"source":["Experiment with different hyperparameters:\n"]},{"cell_type":"markdown","id":"97b30792-0c3c-42bd-985e-be7da2764d04","metadata":{},"outputs":[],"source":["## Exercise 5: Epochs and Batch Size\n","Instruction:\n","Read the description below and complete the code and questions that follow.\n","\n","`Epoch` refers to one complete pass through the entire training dataset.\n","\n","`Batch size` is the number of training samples utilized in one iteration. If the batch size is equal to the total number of samples in the training set, then every epoch has one iteration. In Stochastic Gradient Descent, the batch size is set to one. A batch size of 32--512 data points seems like a good value, for more information check out the following <a href=\"https://arxiv.org/abs/1609.04836?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-cvstudio-2021-01-01\">link</a>.\n","\n","```python\n","# Set the number of epochs and batch size\n","n_epochs = ____     # A typical small number like 10\n","batch_size = ____   # Try 32\n","```\n"]},{"cell_type":"code","id":"4f4674d7-2c96-49e7-a8c2-7d3ac0928804","metadata":{},"outputs":[],"source":["# Write your response."]},{"cell_type":"markdown","id":"5b0a21ae-8a1d-41f0-b842-ff478961839d","metadata":{},"outputs":[],"source":["## Exercise 6: Configure Learning Rate and Momentum\n","Instruction:\n","When training a neural network, two essential hyperparameters influence how weights are updated during optimization:\n","\n","`Learning rate` is used in the training of neural networks. Learning rate is a hyperparameter with a small positive value, often in the range between 0.0 and 1.0.\n","\n","`Momentum` (momentum): Helps accelerate gradients in the right direction, improving convergence and stability.\n","\n","Complete the code below with appropriate values for training:\n","\n","```python\n","\n","# Set optimizer hyperparameters\n","learning_rate = _______     # Example: 0.000001\n","momentum = _______          # Example: 0.9\n","```\n"]},{"cell_type":"code","id":"0172a790-3926-44d2-94b2-42a6f5523153","metadata":{},"outputs":[],"source":["# Write your response."]},{"cell_type":"markdown","id":"136c6d7a-692b-41fb-a3f4-5869fbed8249","metadata":{},"outputs":[],"source":["## Exercise 7: Learning Rate Scheduler â€“ Understanding and Setup\n","Instruction:\n","Sometimes, instead of keeping the learning rate constant throughout training, we use a learning rate scheduler that gradually adjusts the learning rate. This can help improve convergence and generalization.\n","\n","In this example, the learning rate will vary between a minimum (base_lr) and maximum (max_lr) over training iterations.\n","Complete the following lines to configure the learning rate scheduler settings:\n","```python\n","# Enable learning rate scheduling\n","lr_scheduler = _____          # Set True or False to control usage\n","base_lr = _____               # Minimum learning rate (e.g., 0.001)\n","max_lr = _____                # Maximum learning rate (e.g., 0.01)\n","```\n"]},{"cell_type":"code","id":"3ded8af4-8cf9-49e8-bee9-813022c69e86","metadata":{},"outputs":[],"source":["# Write your response."]},{"cell_type":"markdown","id":"13d4f7ce-9f2d-4148-adcc-124841106cd4","metadata":{},"outputs":[],"source":["## Exercise 8: Analyze Training Configuration\n","Reflect on the training setup you've just defined and answer the following questions:\n","\n","1. What is the purpose of choosing a specific number of epochs?\n","<details><summary>Click here for Hints</summary>\n","    Think about underfitting vs. overfitting.    \n","</details> \n","\n","\n","2. Why is it important to choose an appropriate batch size?\n","\n","   <details><summary>Click here for Hints</summary>\n","    Consider the impact on model stability, convergence speed, and memory usage.    \n","    </details> \n","\n","3. How does a learning rate scheduler help model training?\n","   <details><summary>Click here for Hints</summary>\n","   Think about how the learning rate affects the size of each update during training.    \n","   </details> \n"]},{"cell_type":"code","id":"854adb5c-837c-43db-82fc-894c6c5b57f3","metadata":{},"outputs":[],"source":["## Write your response here and convert the cell to a markdown."]},{"cell_type":"markdown","id":"5f6a7d7f-c131-40ca-88d4-2846e177cd5f","metadata":{},"outputs":[],"source":["# Load Model and Train\n"]},{"cell_type":"markdown","id":"664379ff-37b4-467b-b2d1-1e9f36193399","metadata":{},"outputs":[],"source":["## Exercise 9: Load a Pretrained Model\n","Instruction: Load the ResNet-18 model from torchvision.models and set it up as a pretrained model.\n","\n","```python\n","\n","# Set the parameter pretrained to true.\n","model = models.resnet18(pretrained=____)\n","```\n"]},{"cell_type":"code","id":"ccb3ddd9-d48a-4133-a711-bf9b0e19319d","metadata":{},"outputs":[],"source":["# Write your response."]},{"cell_type":"markdown","id":"528eed15-bc44-46ba-b338-8fcf19a523ee","metadata":{},"outputs":[],"source":["## Exercise 10: Freeze Feature Extractor Layers\n","Instruction: Set the requires_grad flag to False for all parameters in the model so that they are not updated during training.\n","\n","```python\n","\n","# Freeze all the parameters to prevent updates during training\n","for param in model.________:\n","    param.________ = _______\n","```\n"]},{"cell_type":"code","id":"5b192fd6-e70c-45a8-bdcf-d00d6ee1e853","metadata":{},"outputs":[],"source":["# Write your response."]},{"cell_type":"markdown","id":"85314fcc-9ddc-44f8-b299-9959e54b9c26","metadata":{},"outputs":[],"source":["## Exercise 11: Determine the Number of Classes\n","Instruction: Use the loaded training dataset to determine how many output classes your model needs to predict.\n","\n","```python\n","\n","# Find number of classes from the dataset\n","n_classes = len(train_dataset.______)\n","print(n_classes)\n","```\n"]},{"cell_type":"code","id":"d123d633-9e17-466f-984c-ec7fe88a0a4c","metadata":{},"outputs":[],"source":["# Write your response."]},{"cell_type":"markdown","id":"09c5d94b-0529-4df2-98b8-1ef3c5f4e5ab","metadata":{},"outputs":[],"source":["## Exercise 12: Replace the Output Layer\n","Instruction: Modify the final fully connected layer (model.fc) of the ResNet-18 model to output the correct number of classes.\n","\n","```python\n","\n","# Replace the output layer to match the number of classes\n","model.fc = nn.Linear(___, ___)\n","```\n","<details><summary>Click here for Hints</summary>\n"," ResNet-18â€™s final hidden layer has 512 neurons as input to model.fc.\n","</details>\n"]},{"cell_type":"code","id":"7956b43f-97a4-4c1f-bb5f-d996a4f418bd","metadata":{},"outputs":[],"source":["# Write your response."]},{"cell_type":"markdown","id":"09ae62e5-b2d3-4591-a54c-9039477fa392","metadata":{},"outputs":[],"source":["Set device type\n"]},{"cell_type":"code","id":"bdc3f20b-0ca8-40be-bd57-ff86cb6660ef","metadata":{},"outputs":[],"source":["model.to(device)"]},{"cell_type":"markdown","id":"4094fa5d-8c4f-4bb7-958f-a7fe1ddc27c7","metadata":{},"outputs":[],"source":["## Exercise 13: Define the Loss Function\n","Instruction: Set up the loss function to be used for training a multi-class classifier.\n","\n","```python\n","\n","# Define the loss function for multi-class classification\n","criterion = nn.____________________()\n","```\n","<details><summary>Click here for Hints</summary>        \n","Cross-entropy loss (also called log loss) is commonly used for classification problems where the target variable can take on one of C classes. It internally applies LogSoftmax, so you donâ€™t need to apply a softmax layer separately before the loss.    \n","    \n","</details>\n"]},{"cell_type":"code","id":"d9f85f9a-235a-4c32-86fc-d7b1033c0881","metadata":{},"outputs":[],"source":["# Write your response."]},{"cell_type":"markdown","id":"fa001174-2211-4142-bc26-351170a9dd95","metadata":{},"outputs":[],"source":["## Exercise 14: Create Data Loaders\n","Instruction: Initialize the data loaders to feed data in batches during training and evaluation.\n","\n","```python\n","\n","# Create data loaders for training and validation\n","train_loader = torch.utils.data.DataLoader(dataset=__________, batch_size=______, shuffle=True)\n","validation_loader = torch.utils.data.DataLoader(dataset=__________, batch_size=1)\n","```\n","<details><summary>Click here for Hints</summary>\n","For validation, a batch size of 1 is often used when you want to evaluate each image individually and get fine-grained performance insights.\n","</details>\n"]},{"cell_type":"code","id":"239aa7ec-7b4d-4d60-a045-7977c3056d6e","metadata":{},"outputs":[],"source":["# Write your response."]},{"cell_type":"markdown","id":"d98c9e83-b31f-4abe-8c0d-21f7ee85390f","metadata":{},"outputs":[],"source":["## Exercise 15: Set Up the Optimizer\n","Instruction: Use the SGD optimizer to update the model parameters during training.\n","\n","```python\n","\n","# Initialize the optimizer\n","optimizer = torch.optim.SGD(model.___________(), lr=______, momentum=______)\n","```\n","<details><summary>Click here for Hints</summary>\n"," Use model.parameters() to pass trainable weights, and set learning_rate and momentum.\n","</details>\n"]},{"cell_type":"code","id":"10933a40-5f0b-46ae-9826-3197ec26d363","metadata":{},"outputs":[],"source":["# Write your response"]},{"cell_type":"markdown","id":"d4b9dd47-6c33-4af6-a797-857952c115f8","metadata":{},"outputs":[],"source":["## Exercise 16: Configure a Learning Rate Scheduler\n","Instruction: If learning rate scheduling is enabled, set up a cyclical learning rate scheduler.\n","\n","```python\n","\n","# Optional: use learning rate scheduler\n","if lr_scheduler:\n","    scheduler = torch.optim.lr_scheduler.CyclicLR(\n","        optimizer,\n","        base_lr=_____,        # Minimum learning rate e.g 0.001\n","        max_lr=_____,         # Maximum learning rate e.g 0.01\n","        step_size_up=5,\n","        mode=\"triangular2\"\n","    )\n","```\n"]},{"cell_type":"code","id":"9a8e6d04-658d-4ef4-8f62-a9b5a863d701","metadata":{},"outputs":[],"source":["# Write your response"]},{"cell_type":"markdown","id":"b407d308-cfb8-4295-b8cd-f706232099f5","metadata":{},"outputs":[],"source":["### The `train_model()` Function: Step-by-Step Explanation\n","\n","This function handles the training and validation process for a deep learning model, including tracking performance and selecting the best model based on validation accuracy.\n","\n","\n","\n","#### ðŸ”¹ Step 1: Initialize Tracking Variables\n","\n","* Create lists to store:\n","\n","  * Training loss per epoch (`loss_list`)\n","  * Validation accuracy per epoch (`accuracy_list`)\n","* Store the **initial model weights** as a baseline for comparison (`best_model_wts`)\n","\n","\n","\n","#### ðŸ”¹ Step 2: Loop Over Epochs\n","\n","For each epoch (a full pass through the training dataset):\n","\n","\n","#### Training Phase\n","\n","* **Set model to training mode**: `model.train()`\n","\n","* **Loop over batches in `train_loader`**:\n","\n","  * Move input data `(x)` and labels `(y)` to the appropriate device (CPU/GPU)\n","  * Perform forward pass: `z = model(x)`\n","  * Compute loss: `loss = criterion(z, y)`\n","  * Backpropagate: `loss.backward()`\n","  * Update weights: `optimizer.step()`\n","  * Reset gradients: `optimizer.zero_grad()`\n","\n","* **Track loss**: Append batch losses to a sublist, then compute the average and store it in `loss_list`\n","\n","\n","\n","####  Adjust Learning Rate (Optional)\n","\n","* If a learning rate scheduler is defined:\n","\n","  * Call `scheduler.step()` to update the learning rate\n","\n","\n","\n","####  Validation Phase\n","\n","* **Set model to evaluation mode**: `model.eval()`\n","\n","* **Disable gradient calculation**: Use `with torch.no_grad():`\n","\n","* **Loop over `validation_loader`**:\n","\n","  * Run forward pass\n","  * Predict labels using `torch.max()`\n","  * Count correct predictions\n","\n","* **Calculate validation accuracy** and store in `accuracy_list`\n","\n","\n","\n","#### Track and Save the Best Model\n","\n","* If current validation accuracy exceeds previous best:\n","\n","  * Save the model's weights using `copy.deepcopy(model.state_dict())`\n","\n","\n","\n","#### Print Epoch Metrics (Optional)\n","\n","* Print current learning rate, validation loss, and validation accuracy if `print_` is `True`\n","\n","\n","\n","#### Final Step: Load Best Model\n","\n","* After all epochs, load the saved weights corresponding to the best validation performance\n","\n","\n","#### Return Values\n","\n","* Final trained model with best weights\n","* List of validation accuracies (`accuracy_list`)\n","* List of training losses (`loss_list`)\n","\n","\n"]},{"cell_type":"markdown","id":"1e74eb18-a544-4f1c-bfb0-2a1d8e2791ee","metadata":{},"outputs":[],"source":["## Exercise 17: Implement and Analyze the Training Loop\n","Instructions: The function below defines a complete training and evaluation loop for a neural network model using PyTorch. Review the function carefully and complete the missing parts to ensure the model trains, tracks loss and accuracy, and retains the best-performing weights.\n","\n","Complete the code and answer the questions that follow:\n","```python\n","def train_model(model, train_loader, validation_loader, criterion, optimizer, n_epochs, print_=True):\n","    loss_list = []        # Store average training loss per epoch\n","    accuracy_list = []    # Store validation accuracy per epoch\n","    correct = 0\n","\n","    n_test = len(val_dataset)\n","    accuracy_best = 0\n","    best_model_wts = copy.deepcopy(model.________())  # Fill in the blank e.g state_dict()\n","\n","    print(\"The first epoch should take several minutes\")\n","\n","    for epoch in tqdm(range(n_epochs)):\n","        loss_sublist = []\n","\n","        for x, y in train_loader:\n","            x, y = x.to(device), y.to(device)\n","            model.____()   # Set model to training mode\n","\n","            z = model(x)\n","            loss = criterion(z, y)\n","            loss_sublist.append(loss.item())\n","\n","            loss.________()        # Backpropagation\n","            optimizer.________()   # Update weights\n","            optimizer.________()   # Reset gradients\n","\n","        print(f\"Epoch {epoch + 1} done\")\n","\n","        # Step learning rate scheduler (if used)\n","        scheduler.step()\n","\n","        loss_list.append(np.mean(loss_sublist))\n","\n","        # Validation\n","        correct = 0\n","        model.eval()\n","        with torch.no_grad():\n","            for x_test, y_test in validation_loader:\n","                x_test, y_test = x_test.to(device), y_test.to(device)\n","                z = model(x_test)\n","                _, yhat = torch.max(z.data, 1)\n","                correct += (yhat == y_test).sum().item()\n","\n","        accuracy = correct / n_test\n","        accuracy_list.append(accuracy)\n","\n","        if accuracy > accuracy_best:\n","            accuracy_best = accuracy\n","            best_model_wts = copy.deepcopy(model.state_dict())\n","\n","        if print_:\n","            print(\"Learning rate:\", optimizer.param_groups[0]['lr'])\n","            print(f\"Validation loss (epoch {epoch + 1}): {np.mean(loss_sublist):.4f}\")\n","            print(f\"Validation accuracy (epoch {epoch + 1}): {accuracy:.4f}\")\n","\n","    model.load_state_dict(best_model_wts)\n","    return accuracy_list, loss_list, model\n","```\n"]},{"cell_type":"code","id":"fa23a3f4-d6a4-49fb-8c63-5cdfb299d3c5","metadata":{},"outputs":[],"source":["# Write your response."]},{"cell_type":"markdown","id":"4a1a16e5-c2ce-4787-9f13-62cadffee6dd","metadata":{},"outputs":[],"source":["## Exercise 18: Reflection \n","What would happen if we did not track the best model weights during training?\n","\n","<details><summary>Click here for Hints</summary>\n","   During training, the model might perform better on the validation set at earlier epochs before it starts overfitting. If you only keep the weights from the last epoch, you might miss the model that had the best generalization performance.   \n","</details> \n","\n"]},{"cell_type":"code","id":"9601a0dd-2351-41c4-bc6f-a761ba8ab9d3","metadata":{},"outputs":[],"source":["## Write your response here and convert the cell to a markdown."]},{"cell_type":"markdown","id":"5eea98f8-b883-41e7-8ea3-5c2970176154","metadata":{},"outputs":[],"source":["Now we are going to train model,for the images this take 25 minutes, depending on your dataset\n"]},{"cell_type":"code","id":"96f8ce60-889d-46d4-9f65-34785bc83ff6","metadata":{},"outputs":[],"source":["# Start time tracking\nstart_datetime = datetime.now()\nstart_time = time.time()\n\n# Train the model\naccuracy_list, loss_list, model = train_model(\n    model, train_loader, validation_loader, criterion, optimizer, n_epochs=n_epochs\n)\n\n# End time tracking\nend_datetime = datetime.now()\nelapsed_time = time.time() - start_time\n\n# Print results\nprint(\"Training completed.\")\nprint(f\"Start Time     : {start_datetime.strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(f\"End Time       : {end_datetime.strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(f\"Elapsed Time   : {elapsed_time:.2f} seconds\")\n"]},{"cell_type":"markdown","id":"4bc639b5-6cfc-4348-9d7c-79deb51a242c","metadata":{},"outputs":[],"source":["Save the model to model.pt\n"]},{"cell_type":"code","id":"4db186a9-8220-46ad-9e02-7277bef09da6","metadata":{},"outputs":[],"source":["# Save the model to model.pt\ntorch.save(model.state_dict(), 'model.pt')"]},{"cell_type":"markdown","id":"5e90aaee-bdc5-44cb-a984-3a5f9d2e71f9","metadata":{},"outputs":[],"source":["Plot train cost and validation accuracy,  you can improve results by getting more data.\n"]},{"cell_type":"markdown","id":"125563e4-e069-4811-9c6a-e781a77a3190","metadata":{},"outputs":[],"source":["## Exercise 19: Plot Training Loss and Validation Accuracy\n","After training the model, we can visualize its performance using the recorded metrics.\n","\n","Complete the following code to plot the modelâ€™s learning behavior over epochs:\n","\n","```python\n","\n","# Use the stored training loss and validation accuracy to plot learning curves\n","plot_stuff(_____, _____)  # Fill in the appropriate variables\n","```\n","<details><summary>Click here for Hints</summary>\n","These variables were returned by the train_model() function and track loss and accuracy over all epochs.\n","</details>\n"]},{"cell_type":"code","id":"ab7381fd-dc66-4937-be0f-eb5d0e4060bf","metadata":{},"outputs":[],"source":["# Write your response"]},{"cell_type":"markdown","id":"f375615d-1dcc-4e75-b2f8-d4a0e45d6d80","metadata":{},"outputs":[],"source":["## Exercise 20: Interpret the Learning Curves\n","Observe the plot of training loss and validation accuracy over the training iterations:\n","\n","Questions:\n","1. What trend do you observe in the training loss?\n","\n","        A. It increases over time\n","        \n","        B. It decreases steadily\n","        \n","        C. It fluctuates randomly\n","\n","2. What trend do you observe in the validation accuracy?\n","\n","        A. It improves consistently\n","        \n","        B. It fluctuates but generally stays high\n","        \n","        C. It declines gradually\n"]},{"cell_type":"code","id":"57a9121a-24f6-447c-adfb-dc91df652f0d","metadata":{},"outputs":[],"source":["## Write your response here and convert the cell to a markdown. e.g 1.A or 1.B"]},{"cell_type":"markdown","id":"b6b35fc4-40ce-475b-b29d-b0c4f586edd3","metadata":{},"outputs":[],"source":["## Test Our Model with an Uploaded Image\n"]},{"cell_type":"markdown","id":"41fc96b3-fd23-4e7b-b735-5cb7f9c1d71f","metadata":{},"outputs":[],"source":["Upload your image, and see if it will be correctly classified.\n","<p><b>Instructions on How to Upload an Image:</b></p>\n","Use the upload button and upload an image from your local machine:\n","<center>\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images/instruction.png\" width=\"300\"  />\n","</center>\n"]},{"cell_type":"markdown","id":"0710e412-9933-4560-82f5-ebb38b4f9913","metadata":{},"outputs":[],"source":["The image will now be in the directory in which you are working in. To read the image in a new cell, use the <code>cv2.imread</code> and read its name. For example, I uploaded <code>anothercar.jpg</code> into my current working directory - <code>cv2.imread(\"anothercar.jpg\")</code>.\n","\n","<center>\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images/instruction2.png\" width=\"300\"  />\n","</center>\n"]},{"cell_type":"markdown","id":"34ef9dc6-a251-40ba-baeb-7281ec1f0d28","metadata":{},"outputs":[],"source":["Else use the below images to test.\n"]},{"cell_type":"code","id":"78e69039-2093-4ce2-9758-e58c75f61f1d","metadata":{},"outputs":[],"source":["!wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/ko-rMe71oPApYpUj2urgFQ/stop-1.jpeg\"\n!wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/2oSHTMfHikZvnhKypHO9Uw/stop-2.jpeg\"\n!wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/6GVQqlNsZ83-me4L9DzAIg/not-stop-1.jpeg\"\n!wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/47aCgskKGqJTYmIkvV6_mA/not-stop-2.jpeg\""]},{"cell_type":"markdown","id":"142706ef-cf85-4908-bdc8-f0938157e831","metadata":{},"outputs":[],"source":["Define the class name and Load the model\n"]},{"cell_type":"markdown","id":"8a51a8f4-3a97-43c7-b997-f1e9530a662a","metadata":{},"outputs":[],"source":["## Exercise 21: Load a Trained Model for Inference\n","Instruction:\n","You have already trained a ResNet18 model to classify images as either \"stop\" or \"not_stop\". Complete the following code to recreate the architecture and load the saved weights for making predictions.\n","\n","```python\n","\n","# Define class labels used during training\n","# Fill in the class names ['not_stop', 'stop']\n","class_names = ['_____', '_____']  \n","\n","# Load the same pretrained architecture used during training\n","# Should we use pretrained weights here? If no set False\n","model = models.resnet18(pretrained=_____) \n","\n","# Replace the fully connected layer to match number of output classes\n","# Replace with correct number of classes not_stop / stop\n","model.fc = torch.nn.Linear(model.fc.in_features, ___)  \n","\n","# Load trained model weights\n","#saved model \n","model.load_state_dict(torch.load(\"_____\", map_location=torch.device('cpu'))) \n","\n","# Set the model to evaluation mode\n","model.____()  # Inference mode                                                                                                                                                                              \n"]},{"cell_type":"code","id":"afb196fa-fed9-4d36-88ca-c971224c9272","metadata":{},"outputs":[],"source":["# Write your response"]},{"cell_type":"markdown","id":"f307cacc-7c9a-4159-9081-691c20a606d5","metadata":{},"outputs":[],"source":["## Exercise 22: Preprocess an Image for Prediction\n","Instruction:\n","To use a trained model for inference, you must preprocess input images in the same way as during training. This includes resizing, normalizing, and converting the image to a tensor.\n","\n","Complete the code below to prepare a new image (\"not_stop_1.jpeg\") for prediction:\n","\n","```python\n","\n","# Define the same image transformations used during training\n","transform = transforms.Compose([\n","    transforms.____((224, 224)),       # Resize image\n","    transforms.____(),                 # Convert PIL image to tensor\n","    transforms.Normalize([0.485, 0.456, 0.406],   # Normalize using ImageNet stats\n","                         [0.229, 0.224, 0.225])\n","])\n","\n","```\n"]},{"cell_type":"code","id":"77c789d3-f4d4-4d37-ae49-1ae0627ffdfd","metadata":{},"outputs":[],"source":["# Write your response"]},{"cell_type":"markdown","id":"4c855102-325b-47be-b39c-40b2c3bb4757","metadata":{},"outputs":[],"source":["## Exercise 23:Load and convert the image\n","\n","```python\n","image_path = \"_____\"  # Provide image filename e.g stop_1.jpeg\n","image = Image.open(image_path).convert(\"_____\")  # Ensure 3 channels e.g RGB\n","\n","# Apply transformations and add batch dimension e.g 0\n","input_tensor = transform(image).unsqueeze(___)\n","```\n"]},{"cell_type":"code","id":"d9edf0bc-8568-469b-b2f7-c3dadb42a108","metadata":{},"outputs":[],"source":["# Write your response"]},{"cell_type":"markdown","id":"ae63c670-1d80-4256-afb8-ff381e6b4118","metadata":{},"outputs":[],"source":["## Exercise 24:\n","Why do we use unsqueeze(0) after applying transforms to the image?\n","\n","        A. To normalize the image\n","        \n","        B. To convert it into RGB format\n","        \n","        C. To add a batch dimension for the model\n","        \n","        D. To flatten the image tensor\n","\n"]},{"cell_type":"code","id":"ff9369e0-071a-48cb-bca7-418a6debd17a","metadata":{},"outputs":[],"source":["## Write your response here and convert the cell to a markdown. e.g 1.A or 1.B"]},{"cell_type":"markdown","id":"f40e8226-5bfb-4a5b-abaa-4aba55be72c1","metadata":{},"outputs":[],"source":["## Exercise 25: Make a Prediction and Display the Result\n","Instruction:\n","Once the image is preprocessed and passed into the model, you can obtain the predicted class using torch.argmax(). Complete the code to classify and visualize the result.\n","\n","```python\n","\n","# Perform inference without tracking gradients using no_grad()\n","with torch.____():  \n","    outputs = model(input_tensor)  # Forward pass\n","    predicted_class = torch.____(outputs, dim=1).item()  # Get predicted class index using argmax\n","\n","# Display result\n","print(f\"The image was classified as: {class_names[predicted_class]}\")\n","\n","# Visualize the image with the predicted label\n","plt.imshow(image)  # Original image (PIL format)\n","plt.title(f\"Predicted: {class_names[predicted_class]}\")\n","plt.axis(\"_____\")\n","plt.show()\n","```\n"]},{"cell_type":"code","id":"e522652d-cdc4-42d4-a29c-8537c349df1e","metadata":{},"outputs":[],"source":["# Write your response"]},{"cell_type":"markdown","id":"ee283754-aa63-4a31-bfa6-b757ffb5f388","metadata":{},"outputs":[],"source":["### Congratulations! You've made it the end of your final project!\n","\n","You've successfully built and evaluated an image classifier using transfer learningâ€”a powerful and practical approach in modern computer vision. This marks a significant milestone in applying deep learning to real-world tasks.\n"]},{"cell_type":"markdown","id":"5d616e9d-255b-491a-aa1e-a75ebf930c74","metadata":{},"outputs":[],"source":["## Authors\n"]},{"cell_type":"markdown","id":"1a721ff8-344b-450e-abac-7fd2ba0a668d","metadata":{},"outputs":[],"source":["Joseph Santarcangelo\n","\n","[Sathya Priya](https://www.linkedin.com/in/sathya-priya-06120a17a/) \n","\n"]},{"cell_type":"markdown","id":"66b146be-03b5-4e05-8da5-70a84364a9a8","metadata":{},"outputs":[],"source":["<!--## Change Log-->\n"]},{"cell_type":"markdown","id":"da2d64de-45bc-4eba-bde7-eebfca9038db","metadata":{},"outputs":[],"source":["<!--| Date (YYYY-MM-DD) | Version | Changed By | Change Description      |\n","| ----------------- | ------- | ---------- | ----------------------- |\n","| 2025-07-04        | 1.0     | Sathya     | Created the notebook for peer assignment |\n","| 2021-05-25        | 0.3     | Yasmine    | Modifies Multiple Areas |\n","| 2021-05-25        | 0.3     | Kathy      | Modified Multple Areas. |\n","| 2021-03-08        | 0.2     | Joseph     | Modified Multiple Areas |\n","| 2021-02-01        | 0.1     | Joseph     | Modified Multiple Areas |-->\n"]},{"cell_type":"markdown","id":"6a7f39ff-d4f2-4eac-9b2a-f49f19d7a40a","metadata":{},"outputs":[],"source":["<h3 align=\"center\"> &#169; IBM Corporation. All rights reserved. <h3/>\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.8"},"prev_pub_hash":"3819eeb8890a8744cdb27a3c6005232798ec4d309a4748188be22001eb8b7b4d"},"nbformat":4,"nbformat_minor":4}